@book{Corke2011,
	author = {Peter Corke},
	publisher = {Springer Berlin Heidelberg},
	title = {{Robotics, Vision and Control }},
	subtitle = { Fundamental Algorithms in MATLAB},
	year = {2011},
	series = {Springer Tracts in Advanced Robotics}
}

@book{Bundle2016,
	editor = {一般社団法人 日本写真測量学会},
	publisher = {東京電機大学出版局},
	title = {{3次元画像計測の基礎 - バンドル調整の理論と実践 -}},
	year = {2016}
}

@book{深層学習2015,
	editor = {岡谷 貴之},
	publisher = {講談社},
	title = {{深層学習}},
	year = {2015}
	series = {機械学習プロフェッショナルシリーズ}
}


@article{Hoffmann2006,
author = {Hoffmann, Frank and Nierobisch, Thomas and Seyffarth, Thorsten},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/Visual Servoing with Moments of SIFT Features.pdf:pdf},
journal = {Computational Intelligence},
title = {{¨ T DORTMUND UNIVERSIT A REIHE COMPUTATIONAL INTELLIGENCE Visual Servoing with Moments of SIFT Features}},
year = {2006}
}
@misc{Malis1999,
abstract = {We propose an approach to vision-based robot control, called 2?D visual servoing, which avoids the respective drawbacks of classical position-based and image-based visual servoing. Contrary to the position-based visual servoing, our scheme does not need any geometric three-dimensional model of the object. Furthermore and contrary to image-based visual servoing, our approach ensures the convergence of the control law in the whole task space. 2?D visual servoing is based on the estimation of the partial camera displacement from the current to the desired camera poses at each iteration of the control law. Visual features and data extracted from the partial displacement allow us to design a decoupled control law controlling the six camera DOFs. The robustness of our visual servoing scheme with respect to camera calibration errors is also analyzed: the necessary and sufficient conditions for local asymptotic stability are easily obtained. Then, due to the simple structure of the system, sufficient conditions for global asymptotic stability are established. Finally, experimental results with an eye-in-hand robotic system confirm the improvement in the stability and convergence domain of the 2?D visual servoing with respect to classical position-based and image-based visual servoing},
author = {Malis, Ezio and Chaumette, Fran{\c{c}}ois and Boudet, Sylvie},
booktitle = {IEEE Transactions on Robotics and Automation},
doi = {10.1109/70.760345},
isbn = {1042-296X VO - 15},
issn = {1042296X},
number = {2},
pages = {238--250},
title = {{2-1/2-D visual servoing}},
volume = {15},
year = {1999}
}
@article{Kawai2004,
author = {Kawai, Hiroyuki},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kawai - 2004 - 3 9923022204 2004 1.pdf:pdf},
title = {3 9923022204 2004 1},
year = {2004}
}
@article{Stability,
author = {Stability, Visual Servoing},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/ハンドアイ3-Dビジュアルサーボの動的安定性の改善と検証.pdf:pdf},
keywords = {6 dof,hand-eye motion compensation,visual servoing},
pages = {3--6},
title = {{3-D}}
}
@article{,
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2008 - 3d 2008.7.3 2008.pdf:pdf},
title = {{3D 2008.7.3 2008}},
year = {2008}
}
@article{Teuliere2014,
abstract = {This paper proposes a novel model-based tracking approach for 3-D localization. One main difficulty of standard model-based approach lies in the presence of low-level ambiguities between different edges. In this paper, given a 3-D model of the edges of the environment, we derive a multiple hypotheses tracker which retrieves the potential poses of the camera from the observations in the image. We also show how these candidate poses can be integrated into a particle filtering framework to guide the particle set toward the peaks of the distribution. Motivated by the UAV indoor localization problem where GPS signal is not available, we validate the algorithm on real image sequences from UAV flights.},
author = {Teuliere, Celine and Marchand, Eric and Eck, Laurent},
doi = {10.1109/TCYB.2014.2337652},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Teuliere, Marchand, Eck - 2014 - 3-D Model-Based Tracking for UAV Indoor Localization.pdf:pdf},
isbn = {1424466741},
issn = {21682267},
journal = {IEEE Transactions on Cybernetics},
keywords = {Model-based tracking,UAV application,particle filtering},
number = {5},
pages = {869--879},
pmid = {25099967},
title = {{3-D model-based tracking for UAV indoor localization}},
volume = {45},
year = {2015}
}
@inproceedings{Campoy,
author = {Mondrag{\'{o}}n, I.F. and Campoy, Pascual and Marti?nez, C. and Olivares-M{\'{e}}ndez, M.A.},
booktitle = {Robotics and Automation (ICRA), 2010 IEEE International Conference on},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/3D pose estimation based on planar object tracking for UAVs control ICRA2010.pdf:pdf},
pages = {35--41},
title = {{3D pose estimation based on planar object tracking for UAVs control}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5509287},
year = {2010}
}
@article{Words1996a,
author = {細田, 耕 and 阪本, 健二 and 浅田, 稔},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Words - 1996 - 3 !! 85 F9 = @. {\$} r9T {\$} o {\$} J {\$}{\$} k3P {\%} 5 !!{\%}“ 7O {\$} N {\$} {\$} a {\$} N YED.pdf:pdf},
journal = {ロボット学会誌},
keywords = {3,4,5,7,7o,8f,9,bds0f,c,epipolar constraint,f,fc,i,j,k,k3p,kd,kj,kmxmq,l,no 3d reconstruction,obstacle avoidance,p,pjs,r,u,visual servoing,w},
number = {Xx},
pages = {1001--1006},
title = {3次元再構成を行わない視覚サーボ系のための 障害物回避軌道の生成},
volume = {XX},
year = {1996}
}
@article{Jun2007,
author = {Jun, Jinwoo and Kawamura, Sadao and Kanaoka, Katsuya and Ichii, Hiroaki},
doi = {10.7210/jrsj.25.598},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jun et al. - 2007 - A Cascaded Feedback Control Scheme for Trajectory Tracking of Robot Manipulator Systems WITH Actuator Dynamics.pdf:pdf},
issn = {0289-1824},
journal = {Journal of the Robotics Society of Japan},
keywords = {actuator dynamics,cascaded feedback,high-order systems,nonlinear,real-time trajectory track-},
number = {4},
pages = {598--605},
title = {{A Cascaded Feedback Control Scheme for Trajectory Tracking of Robot Manipulator Systems WITH Actuator Dynamics}},
url = {http://joi.jlc.jst.go.jp/JST.Journalarchive/jrsj1983/25.598?from=CrossRef},
volume = {25},
year = {2007}
}
@article{Teuliere2014a,
abstract = {This paper presents a novel 3-D servoing approach using dense depth maps to perform robotic tasks. With respect to position-based approaches, our method does not require the estimation of the 3-D pose (direct), nor the extraction and matching of 3-D features (dense), and only requires dense depth maps provided by 3-D sensors. Our approach has been validated in various servoing experiments using the depth information from a low-cost Red Green Blue-Depth (RGB-D) sensor. Positioning tasks are properly achieved despite noisy measurements, even when partial occlusions or scene modifications occur. We also show that, in cases where a reference depth map cannot be easily available, synthetic ones generated with a rendering engine still lead to satisfactory positioning performances. Application of the approach to the navigation of a mobile robot is also demonstrated.},
author = {Teuliere, Celine and Marchand, Eric},
doi = {10.1109/TRO.2014.2325991},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/A Dense and Direct Approach to Visual Servoing Using depth maps.pdf:pdf},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Dense sensor-based control,depth map,visual servoing},
number = {5},
pages = {1242--1249},
title = {{A dense and direct approach to visual servoing using depth maps}},
url = {http://hal.inria.fr/hal-00991641/},
volume = {30},
year = {2014}
}
@article{Namba2009,
author = {Namba, Hitoshi and {Muneyasu .M.}, Mitsuji},
doi = {10.1109/ISPACS.2009.4806661},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Namba, Muneyasu - 2009 - A detection and tracking method based on POC for oncoming cars.pdf:pdf},
isbn = {9781424425655},
journal = {2008 International Symposium on Intelligent Signal Processing and Communication Systems, ISPACS 2008},
month = {feb},
pages = {1--4},
publisher = {Ieee},
title = {{A detection and tracking method based on POC for oncoming cars}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4806661},
year = {2008}
}
@article{Tamadazte2012,
abstract = {This paper demonstrates an accurate nanopositioning scheme based on a direct visual servoing process. This technique uses only the pure image signal (photometric information) to design the visual servoing control law. With respect to traditional visual servoing approaches that use geometric visual features (points, lines},
author = {Tamadazte, Brahim and Piat, Nadine Le Fort and Marchand, Eric},
doi = {10.1109/TMECH.2011.2128878},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/A Direct Visual Servoing Scheme for Automatic Nanopositioning.pdf:pdf},
issn = {10834435},
journal = {IEEE/ASME Transactions on Mechatronics},
keywords = {Micromanipulation,microrobotics,visual servoing},
number = {4},
pages = {728--736},
title = {{A direct visual servoing scheme for automatic nanopositioning}},
volume = {17},
year = {2012}
}
@inproceedings{Zang2012,
author = {Zang, Chuantao and Hashimoto, Koichi},
booktitle = {2012 IEEE International Conference on Mechatronics and Automation, ICMA 2012},
doi = {10.1109/ICMA.2012.6283163},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/A Flexible Camera Positioning Strategy for Robot-based Visual Inspection Application.pdf:pdf},
isbn = {9781467312776},
pages = {527--532},
title = {{A flexible camera positioning strategy for robot-based visual inspection applications}},
year = {2012}
}
@article{Zhang2009,
abstract = {We propose a flexible new technique to easily calibrate a camera. It is well suited for use without specialized knowledge of 3D geometry or computer vision. The technique only requires the camera to observe a planar pattern shown at a few (at least two) different orientations. Either the camera or the planar pattern can be freely moved. The motion need not be known. Radial lens distortion is modeled. The proposed procedure consists of a closed-form solution, followed by a nonlinear refinement based on the maximum likelihood criterion. Both computer simulation and real data have been used to test the proposed technique, and very good results have been obtained. Compared with classical techniques which use expensive equipment such as two or three orthog- onal planes, the proposed technique is easy to use and flexible. It advances 3D computer vision one step from laboratory environments to real world use.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Zhang, Zhengyou},
doi = {10.1109/34.888718},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/A Flexible New Technique for Camera Calibration.pdf:pdf},
isbn = {MSR-TR-98-71},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {11},
pages = {1330--1334},
pmid = {131},
title = {{A Flexible New Technique for Camera Calibration (Technical Report)}},
volume = {22},
year = {2002}
}
@article{Correlation,
author = {Correlation, Using Phase-only},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Correlation - Unknown - A High-Accuracy Dental Radiograph Registration Algorithm.pdf:pdf},
title = {{A High-Accuracy Dental Radiograph Registration Algorithm}}
}
@article{Nagashima,
abstract = {This paper proposes a high-accuracy rotation estimation al- gorithm using 1D Phase-Only Correlation (POC). In general, the rota- tion angle between two images is estimated as follows: (i) convert the image rotation into the image shift by polar mappings of the amplitude spectra of images, and (ii) estimate the translational displacement be- tween the polar mappings to obtain the rotation angle. The problem of rotation estimation between two images is replaced to 1D displacement estimation between pairs of horizontal lines at the same vertical posi- tion in two polar mappings. The proposed algorithm employs 1D POC instead of 2D matching for estimating a rotation angle. The use of 1D POC to estimate the rotation angle makes it possible to reduce the com- putational cost significantly without sacrificing the estimation accuracy.},
author = {Nagashima, Sei and Ito, Koichi and Aoki, Takafumi and Ishii, Hideaki},
doi = {10.1007/978-3-540-74260-9_19},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nagashima et al. - Unknown - High-Accuracy Rotation Estimation Algorithm Based on 1D Phase-Only Correlation and Its Evaluation.pdf:pdf},
isbn = {978-3-540-74258-6},
journal = {Image Analysis and Recognition (Lecture Notes in Computer Science)},
pages = {210--221},
title = {{A High-Accuracy Rotation Estimation Algorithm Based on 1D Phase-Only Correlation}},
volume = {4633},
year = {2007}
}
@article{Shiraishi,
author = {Shiraishi, Takayuki and Fujimoto, Hiroshi},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shiraishi, Fujimoto - Unknown - A Method of Trajectory Tracking Control for Nonminimum Phase Continuous Time Systems S y k u k S y.pdf:pdf},
pages = {1--6},
title = {{A Method of Trajectory Tracking Control for Nonminimum Phase Continuous Time Systems S y [ k ] u [ k ] S y [ k ]}}
}
@article{Zhang2003,
abstract = {This paper investigates the problem of how to carry out 3D scene reconstruction$\backslash$nfrom multiple views. It improves the algorithms of projective reconstruction$\backslash$nbased on the homography induced by the infinite plane which should$\backslash$nhave 4 points on a reference plane visible in all views given by$\backslash$nHartley and Rother et al., and proposes a new linear algorithm based$\backslash$non 3 points on a reference plane visible in all views. It avoids$\backslash$nthe difficult task of determining whether 4 object points are coplanar$\backslash$nor not, because 3 points which are not collinear just determine a$\backslash$nplane.},
author = {Zhang, Quan-Ting and Wang, Hai-Xian and Wei, Sui},
doi = {10.1109/ICMLC.2003.1260052},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/A NEW ALGORITHM FOR 3D PROJECTIVE RECONSTRUCTION BASED ON INFINITE HOMOGRAPHY.pdf:pdf},
isbn = {0780378652},
journal = {Machine Learning and Cybernetics, 2003 International Conference on},
keywords = {3D projective reconstruction; 3D scene reconstruct},
number = {November},
pages = {2882--2886},
title = {{A new algorithm for {\{}3D{\}} projective reconstruction based on infinite homography}},
url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1260052},
volume = {5},
year = {2003}
}
@article{Corke2000,
abstract = {In image-based visual servo control, where control is effected with respect to the image, there is no direct control over the Cartesian velocities of the robot end effector. As a result, the robot executes trajectories that are desirable in the image, but which can be indirect and seemingly contorted in Cartesian space. We describe the cause of these phenomena, and introduce a new partitioned approach to visual servo control that overcomes the problem. In particular, we decouple the z-axis rotational and translational components of the control from the remaining degrees of freedom. Then, to guarantee that all features remain in the image throughout the entire trajectory, we incorporate a potential function that repels feature points from the boundary of the image plane. We illustrate our control scheme with a variety of simulation results},
author = {Corke, P.I. and Hutchinson, S.a.},
doi = {10.1109/CDC.2000.914182},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/A New Hybrid Image-Based Visual Servo Control Scheme.pdf:pdf},
isbn = {0-7803-6638-7},
issn = {0191-2216},
journal = {IEEE Conference on Decision and Control},
keywords = {Cameras,End effectors,Jacobian matrices,Motion control,Orbital robotics,Robot kinematics,Servomechanisms,Servosystems,Space technology,Velocity control},
pages = {2521--2526},
title = {{A new hybrid image-based visual servo control scheme}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=914182},
volume = {3},
year = {2000}
}
@article{Dame2011,
abstract = {In this paper we propose a new way to achieve a navigation task for a non-holonomic vehicle. We consider an image-based navigation process. We show that it is possible to navigate along a visual path without relying on the extraction, matching and tracking of geometric visual features such as keypoint. The new proposed approach relies directly on the information (entropy) contained in the image signal. We show that it is possible to build a control law directly from the maximisation of the shared information between the current image and the next key image in the visual path. The shared information between those two images are obtained using mutual information that is known to be robust to illumination variations and occlusions. Moreover the generally complex task of features extraction and matching is avoided. Both simulations and experiments on a real vehicle are presented and show the possibilities and advantages offered by the proposed method.},
author = {Dame, Amaury and Marchand, Eric},
doi = {10.1109/ICRA.2011.5979546},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dame, Marchand - 2011 - A new information theoretic approach for appearance-based navigation of non-holonomic vehicle.pdf:pdf},
isbn = {9781612843865},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
month = {may},
pages = {2459--2464},
publisher = {Ieee},
title = {{A new information theoretic approach for appearance-based navigation of non-holonomic vehicle}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5979546},
year = {2011}
}
@article{Jin2008,
abstract = {This paper presents an image tracking algorithm for surveillance system, electro-optical tracking system and missile image seeker based on phase correlation and Fourier-Mallin transform. The algorithm consists of an image pre-processing module, a translation estimation module using phase correlation, a fine motion estimation module applied when confidence rate comes from correlation output can not fulfill threshold value, and reference image update module. The fine motion estimation can be used for measuring shift, rotate and scale between a reference and input image based on Fourier-Mellin transform. A sub-pixel registration method is used to enhance estimation accuracy. Proposed algorithm was evaluated its accuracy and robustness using some real indoor and outdoor image sequences.},
author = {Jin, Sang H. and Koh, Gwang S.},
doi = {10.1109/ICCAS.2008.4694650},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jin, Koh - 2008 - A robust image tracker based on phase correlation and fourier-mallin transform.pdf:pdf},
isbn = {9788995003893},
journal = {2008 International Conference on Control, Automation and Systems, ICCAS 2008},
keywords = {Fourier-mellin transform,Phase correlation,Sub-pixel registration,Target tracker},
number = {7},
pages = {1028--1031},
title = {{A robust image tracker based on phase correlation and fourier-mallin transform}},
volume = {2},
year = {2008}
}
@article{Paschall2008,
abstract = {Abstract?The return of humans to the Moon will require increased capability beyond that of the previous Apollo missions. Longer stay times and a greater flexibility with regard to landing locations are among the many improvements planned. A descent and landing system that can land the vehicle more accurately than Apollo with a greater ability to detect and avoid hazards is essential to the development of a Lunar Outpost, and also for increasing the number of potentially accessible Lunar sortie locations. This descent and landing system should allow landings in more challenging terrain and provide more flexibility with regard to mission timing and lighting considerations, while maintaining safety as the top priority. The lunar landing system under development by the ALHAT (Autonomous Landing and Hazard Avoidance Technology) project is addressing this by providing terrain-relative navigation measurements to enhance global-scale precision, an onboard hazard detection system to select safe landing locations, and an Autonomous GNC (Guidance, Navigation, and Control) capability to process these measurements and safely direct the vehicle to a landing location. This landing system will enable safe and precise lunar landings without requiring lunar infrastructure in the form of navigation aids or a priori identified hazard-free landing locations. The safe landing capability provided by ALHAT uses onboard active sensing to detect hazards that are large enough to be a danger to the vehicle but too small to be detected from orbit a priori. Algorithms to interpret raw active sensor terrain data and generate hazard maps as well as identify safe sites and recalculate new trajectories to those sites are included as part of the ALHAT System. These improvements to descent and landing will help contribute to repeated safe and precise landings for a wide variety of terrain on the Moon. 1},
author = {Paschall, Stephen C. and Brady, Tye and Cohanim, Babak E. and Sostaric, Ronald},
doi = {10.1109/AERO.2008.4526298},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/NASA1.pdf:pdf},
isbn = {1424414881},
issn = {1095323X},
journal = {IEEE Aerospace Conference Proceedings},
title = {{A self contained method for safe {\&} precise lunar landing}},
year = {2008}
}
@article{Inoue2013,
author = {Inoue, Toshiaki},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Inoue - 2013 - A Study on Feature-Extraction Methods for Improvement of Image-Recognition Performance.pdf:pdf},
pages = {57--62},
title = {{A Study on Feature-Extraction Methods for Improvement of Image-Recognition Performance}},
volume = {22},
year = {2013}
}
@article{Gans2002,
abstract = {In the recent past, many researchers have developed control algorithms for visual servo applications. In this paper we introduce a new switching approach, in which a high-level decision maker determines which of two low- level visual servo controllers should be used at each control cycle. We introduce two new low-level controllers, one that relies on the homography between initial and goal images, and one that uses an affine transformation to approximate the motion between initial and goal camera configurations. Since an affine transformation can only approximate a restricted set of camera motions, this choice of low-level controllers illustrates the strength of the switching approach. We have evaluated our approach with several simulations using three candidate switching rules. Although our results are preliminary we believe that the proposed method is very promising for visual servo tasks in which there is a significant distance between the initial and goal configurations.},
author = {Gans, Nr and Hutchinson, Sa},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/A Switching Approach to Visual Servo Control.pdf:pdf},
journal = {IEEE International Symposium on Intelligent Control (ISIC)},
pages = {770--776},
title = {{A switching approach to visual servo control}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1157859},
year = {2002}
}
@article{Burges1998,
abstract = {The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Burges, Christopher J.C.},
doi = {10.1023/A:1009715923555},
editor = {Fayyad, Usama},
eprint = {1111.6189v1},
institution = {Bell Laboratories, Lucent Technologies},
isbn = {0818672404},
issn = {13845810},
journal = {Data Mining and Knowledge Discovery},
keywords = {pattern recognition,statistical learning theory,support vector machines,vc dimension},
number = {2},
pages = {121--167},
pmid = {5207842081938259593},
publisher = {Springer},
series = {NetGames '06},
title = {{A Tutorial on Support Vector Machines for Pattern Recognition}},
url = {http://www.springerlink.com/index/Q87856173126771Q.pdf{\%}5Cnhttp://link.springer.com/10.1023/A:1009715923555},
volume = {2},
year = {1998}
}
@article{Hutchinson1996,
author = {Hutchinson, S. and Hager, G.D. and Corke, P.I.},
doi = {10.1109/70.538972},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hutchinson, Hager, Corke - 1996 - A tutorial on visual servo control.pdf:pdf},
issn = {1042296X},
journal = {IEEE Transactions on Robotics and Automation},
number = {5},
pages = {651--670},
title = {{A tutorial on visual servo control}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=538972},
volume = {12},
year = {1996}
}
@misc{Hutchinson1996a,
abstract = {This article provides a tutorial introduction to visual servo control of robotic manipulators. Since the topic spans many disciplines our goal is limited to providing a basic conceptual framework. We begin by reviewing the prerequisite topics from robotics and computer vision, including a brief review of coordinate transformations, velocity representation, and a description of the geometric aspects of the image formation process. We then present a taxonomy of visual servo control systems. The two major classes of systems, position-based and image-based systems, are then discussed in detail. Since any visual servo system must be capable of tracking image features in a sequence of images, we also include an overview of feature-based and correlation-based methods for tracking. We conclude the tutorial with a number of observations on the current directions of the research field of visual servo control},
author = {Hutchinson, Seth a and Hager, G.D. Gregory D. and Corke, Peter I. P.I. and {Hutchinson, Seth}},
booktitle = {IEEE Transactions on Robotics and Automation},
doi = {10.1109/70.538972},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - TutorialOnVisualServoControl.pdf.pdf:pdf},
issn = {1042296X},
pages = {651--670},
title = {{A tutorial on visual servo control}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4015997{\%}5Cnhttp://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=538972{\%}5Cnhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=538972},
volume = {12},
year = {1996}
}
@inproceedings{Malis2005a,
abstract = {In this paper, we present a generic and flexible system for vision-based robot control. The system integrates visual tracking and visual servoing approaches in a unifying framework. In this framework, the generality is obtained using a template matching algorithm based on an efficient second-order minimization. Contrarily to feature-based visual servoing schemes, we avoid the design of feature-dependent visual tracking algorithms. By integrating the visual tracking process with the visual servoing techniques, we can easily deal with constrained tasks. This reduces the computation cost and improves the precision of the system. The experimental results prove the efficiency of the unified system in real conditions. ?? 2005 Elsevier B.V. All rights reserved.},
author = {Malis, Ezio and Benhimane, Selim},
booktitle = {Robotics and Autonomous Systems},
doi = {10.1016/j.robot.2005.03.014},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/A unified approach to visual tracking and servoing.pdf:pdf},
issn = {09218890},
keywords = {Efficient second-order minimization,Real-time,Visual servoing,Visual tracking},
number = {1},
pages = {39--52},
title = {{A unified approach to visual tracking and servoing}},
volume = {52},
year = {2005}
}
@inproceedings{Malis2005,
abstract = {In this paper, we present a generic and flexible system for vision-based robot control. The system integrates visual tracking and visual servoing approaches in a unifying framework. In this framework, the generality is obtained using a template matching algorithm based on an efficient second-order minimization. Contrarily to feature-based visual servoing schemes, we avoid the design of feature-dependent visual tracking algorithms. By integrating the visual tracking process with the visual servoing techniques, we can easily deal with constrained tasks. This reduces the computation cost and improves the precision of the system. The experimental results prove the efficiency of the unified system in real conditions. ?? 2005 Elsevier B.V. All rights reserved.},
author = {Malis, Ezio and Benhimane, Selim},
booktitle = {Robotics and Autonomous Systems},
doi = {10.1016/j.robot.2005.03.014},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/A unified approach to visual tracking and servoing.pdf:pdf},
issn = {09218890},
keywords = {Efficient second-order minimization,Real-time,Visual servoing,Visual tracking},
number = {1},
pages = {39--52},
title = {{A unified approach to visual tracking and servoing}},
volume = {52},
year = {2005}
}
@inproceedings{Avanzini2010,
abstract = {Automated electric vehicles for public use constitute a promising very efficient and environment-friendly "urban transportation system". An additional functionality that could enhance this transportation service is vehicle platooning. In order to avoid inter-distance oscillations within the platoon, a global control strategy, supported by inter-vehicle communications, is investigated. Vehicle localization in an absolute frame is needed and is derived here from monocular vision. The vision data is however expressed in a virtual world, slightly distorted with respect to the actual metric one. It is shown that such a distortion can accurately be corrected by designing a nonlinear observer that relies on odometric data. A global decentralized control strategy, relying on nonlinear control techniques, can then be designed to achieve accurate vehicle platooning. Simulations and full-scale experiments demonstrate the performance of the proposed approach. {\textcopyright}2010 IEEE.},
author = {Avanzini, P. and Thuilot, B. and Martinet, P.},
booktitle = {IEEE/RSJ 2010 International Conference on Intelligent Robots and Systems, IROS 2010 - Conference Proceedings},
doi = {10.1109/IROS.2010.5650018},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/Accurate platoon control of urban vehicles, based solely on monocular vision.pdf:pdf},
isbn = {9781424466757},
issn = {2153-0858},
keywords = {Automatic guided vehicles,Monocular vision,Nonlinear control,Observer,Platooning,Urban vehicles},
pages = {6077--6082},
title = {{Accurate platoon control of urban vehicles, based solely on monocular vision}},
year = {2010}
}
@article{Dame2010,
author = {Dame, A and Marchand, E},
doi = {10.1109/ISMAR.2010.5643550},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dame, Marchand - 2010 - Accurate real-time tracking using mutual information.pdf:pdf},
isbn = {978-1-4244-9343-2},
journal = {IEEE International Symposium on Mixed and Augmented Reality},
month = {oct},
pages = {47--56},
publisher = {Ieee},
title = {{Accurate real-time tracking using mutual information}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5643550},
year = {2010}
}
@article{Kaneishi,
author = {Kaneishi, Takashi and Takahasi, Yasutake and Suzuki, Sho and Asada, Minoru},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaneishi et al. - Unknown - Acquisition of Homing Behaviour of a Mobile Robot Based on Reinforcement Learning .pdf:pdf},
pages = {7--10},
title = {{Acquisition of Homing Behaviour of a Mobile Robot Based on Reinforcement Learning .}}
}
@article{Matthews,
abstract = {Active Appearance Models (AAMs) and the closely related concepts of Morphable Models and Active Blobs are generative models of a certain visual phenomenon. Although linear in both shape and appearance, overall, AAMs are nonlinear parametric models in terms of the pixel intensities. Fitting an AAM to an image consists of minimising the error between the input image and the clos- est model instance; i.e. solving a nonlinear optimisation problem. We propose an efficient fitting algorithm for AAMs based on the inverse compositional image alignment algorithm. We show that the effects of appearance variation during fitting can be precomputed (projected out) using this algorithm and how it can be extended to include a global shape normalising warp, typically a 2D similarity transformation. We evaluate our algorithm to determine which of its novel aspects improve AAM fitting performance.},
author = {Matthews, Iain and Baker, Simon},
doi = {10.1023/B:VISI.0000029666.37597.d3},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/Active Appearance Models Revisited.pdf:pdf},
isbn = {0920-5691},
issn = {0920-5691},
journal = {Int. J. Comput. Vis.},
keywords = {aams,active appearance models,active blobs,efficiency,fitting,gauss-,inverse compositional image alignment,morphable models,newton gradient descent},
number = {2},
pages = {135--164},
title = {{Active Appearance Models Revisited}},
url = {http://link.springer.com/10.1023/B:VISI.0000029666.37597.d3},
volume = {60},
year = {2004}
}
@article{Tang2014,
abstract = {In the complex natural background, the image features of spatial dynamic objects usually change severely, so target recognition method based on single feature could not adapt to the recognition requirements. Due to unpredictable changes of target distance, finding real-time solution of image Jacobi matrix is more difficult, so as achieving active visual servo of spatial dynamic target. Inspired by the visual characteristics of frog eye and bionic recognition mechanism, a bionic spatial-temporal fusion recognition algorithm of dynamic target was provided, and a distance estimation method based on monocular vision information was designed, finally an active visual servo system adapted to three-dimensional tracking applications was established. The physical experiment results show that the bionic recognition method inhibited the background information effectively and enhanced moving target information in multidimension, which is better than the method based on single feature. By using the target distance estimation method, the system solved calculation problem of active vision image Jacobi matrix and fulfilled the requirements of active three-dimensional visual servo.},
author = {Tang, Xiaogang and Wang, Sun'An and Di, Hongyu and Liu, Litian},
doi = {10.1109/CISP.2014.7003825},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/Active Visual Servo Based On Bionic Multi-feature fusion recognition and distance estimation.pdf:pdf},
isbn = {9781479958351},
journal = {Proceedings - 2014 7th International Congress on Image and Signal Processing, CISP 2014},
keywords = {Image Jacobi matrix,active visual servo,bionic multi-feature fusion,distance estimation,dynamic target recognitio},
number = {51375368},
pages = {463--468},
title = {{Active visual servo based on bionic multi-feature fusion recognition and distance estimation}},
year = {2014}
}
@article{Hosoda1996,
author = {Hosoda, Koh and Asada, Minoru},
doi = {10.7210/jrsj.14.313},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hosoda, Asada - 1996 - Adaptive Visual Servoing Controller with Feedforward Compensator without Knowledge of True Jacobian x ! J J {\$} NBh.pdf:pdf},
issn = {0289-1824},
journal = {Journal of the Robotics Society of Japan},
keywords = {adaptive visual servoing,feedforward,on-line estimator,uncalibrated camera-manipulator systems},
number = {2},
pages = {313--319},
title = {{Adaptive Visual Servoing Controller with Feedforward Compensator without Knowledge of True Jacobian.}},
volume = {14},
year = {1996}
}
@article{SrinivasaReddy1996,
abstract = {This correspondence discusses an extension of the well-known phase correlation technique to cover translation, rotation, and scaling. Fourier scaling properties and Fourier rotational properties are used to find scale and rotational movement. The phase correlation technique determines the translational movement. This method shows excellent robustness against random noise.},
author = {{Srinivasa Reddy}, B. and Chatterji, B. N.},
doi = {10.1109/83.506761},
file = {:C$\backslash$:/Users/yoshi/Pictures/forBlog/An FFT Based Technique for Translation Rotation and Scale Invariant Image Registration.pdf:pdf},
isbn = {1057-7149},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
number = {8},
pages = {1266--1271},
pmid = {18285214},
title = {{An FFT-based technique for translation, rotation, and scale-invariant image registration}},
volume = {5},
year = {1996}
}
@article{Ohtsuki2001,
author = {Ohtsuki, Hiroyuki and Aoki, Takafumi and Higuchi, Tatsuo and Kobayashi, Koji},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ohtsuki et al. - 2001 - An Image Matching Algorithm Based on Rotation Invariant Phase-Only Correlation and Its Evaluation.pdf:pdf},
journal = {Sice},
keywords = {POC},
title = {{An Image Matching Algorithm Based on Rotation Invariant Phase-Only Correlation and Its Evaluation}},
year = {2001}
}
@article{Ye2012,
author = {Ye, Tong and Arai, S. and Hashimoto, K.},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/An RC Helocopter Autonomous Control System with Single Web Camera.pdf:pdf},
isbn = {9781467322591},
journal = {2012 Proceedings of SICE Annual Conference (SICE)},
pages = {2127 -- 2132},
title = {{An RC helicopter autonomous control system with single web camera}},
year = {2012}
}
@article{Endo,
author = {Endo, Hiroyuki},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Endo - Unknown - Analysis of Transient Response Based on Short-Span Seeking Control of Hard Disk Drive with Learning Based.pdf:pdf},
number = {2},
pages = {2--7},
title = {{Analysis of Transient Response Based on Short-Span Seeking Control of Hard Disk Drive with Learning Based}}
}
@article{Saito2009,
author = {Saito, Hiroyuki and Hosoi, Akiyoshi},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Saito, Hosoi - 2009 - APPLICATION OF FOURIER PHASE-ONLY CORRELATION.pdf:pdf},
keywords = {dem shading image,fourier phase-only correlation,geometric correction,landsat tm},
number = {1995},
title = {{Application of Fourier Phase-Only Correlation}},
year = {2009}
}
@article{Introduction2014,
abstract = {A nonlinear image-based visual servo control algorithm for autonomous landing of a fixed wing aircraft is described. The primary sensor system is a vision sensor yielding a sequence of images from which 2D linear and point features of the runway are extracted. The first two phases of a landing maneuver, alignment to the runway and glide (descent to the runway) are treated in the work presented here. The final landing maneuvers, flare to touchdown, and taxiing require additional sensor modalities and are not treated. The proposed control scheme deals with unknown wind conditions and incorporates the full nonlinear dynamics of the airplane. Simulation results based on realistic environmental conditions and measurement noise are presented that validate the control design approach. Manuscript},
author = {{Le Bras}, Florent and Hamel, Tarek and Mahony, Robert and Barat, Christian and Thadasack, Julien},
doi = {10.1109/TAES.2013.110780},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/Approach Maneuvers for Autonomous Landing Using Visual Servo Control.pdf:pdf},
issn = {00189251},
journal = {IEEE Transactions on Aerospace and Electronic Systems},
number = {2},
pages = {1051--1065},
title = {{Approach maneuvers for autonomous landing using visual servo control}},
volume = {50},
year = {2014}
}
@article{Xing2006,
abstract = {Star tracker is an important component in the spacecraft attitude determination system. With the small satellite development, the traditional CCD star tracker could not meet the requirement in the mass, power consumption, and volume, The AFS CMOS based Autonomous Star Tracker (AAST) has been proposed recently. The star tracker contains digital image processing unite and attitude algorithm processing unite so that it can perform ali the functions without external support and provide 3-axis attitude information directly. In this paper, the star tracker prototype is introduced. The design concept of the optics and electronics contribute to the construction of a low mass (1kg), low power (2W), high rate (5Hz) and inexpensive autonomous star tracker. This paper also introduces a multi-vectors attitude determination method which has a simple fixed iterated initial value and at most two times iteration, A novel method is proposed an this paper for the star tracker performances testing, especially for the validation of Lost-In-Space star identification and attitude determination algorithms. The performances of the star tracker prototype have been verified through the real night sky experiments using this method},
author = {Xing, Fei and Dong, Ying and You, Zheng and Zhou, Qin},
doi = {10.1109/ISSCAA.2006.1627697},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xing et al. - 2006 - APS star tracker and attitude estimation.pdf:pdf},
isbn = {0-7803-9395-3},
journal = {2006 1st International Symposium on Systems and Control in Aerospace and Astronautics},
pages = {34--38},
title = {{APS star tracker and attitude estimation}},
year = {2006}
}
@article{Gao2014,
abstract = {The acoustic docking control problem is investigated for the fully-actuated autonomous underwater vehicle (AUV) equipped with the USBL transceiver, which provides the positions of the two transponders on the dock station. Similar to the image-based visual servo control technology, the dynamics of the transponders' positions is modeled with the AUV's linear and angular velocities as the control inputs. The docking control errors are defined with the coordinates of the transponders in the local body-fixed frame. The backstepping based adaptive control is utilized to ensure the docking errors to be asymptotically stable with feedback control and adaptation laws. The performances of the proposed docking control are examined by the simulation studies.},
author = {Gao, Jian and Liu, Changxin and Wang, Yingxiang},
doi = {10.1109/OCEANS.2014.7002984},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/Backstepping adaptive docking control for a full-actuated autonomous underwater vehicle with onboard USBL system.pdf:pdf},
isbn = {9781479949182},
journal = {2014 Oceans - St. John's, OCEANS 2014},
keywords = {adaptive docking control,autonomous underwater vehicle,backstepping,onboard USBL},
number = {51279164},
title = {{Backstepping adaptive docking control for a full-actuated autonomous underwater vehicle with onboard USBL system}},
year = {2015}
}
@article{Liu2015,
abstract = {Homography estimation is a fundamental problem in the field of computer vision. For estimating the between two images, one of the key issues is to match keypoints in the reference image to the keypoints in the moving image. To match keypoints in real time, a binary image descriptor, due to its low matching and storage costs, emerges as a more and more popular tool. Upon achieving the low costs, the binary descriptor sacrifices the discriminative power of using floating points. In this paper, we present BB-Homography, a new approach that fuses fast binary descriptor matching and bipartite graph for homography estimation. Starting with binary descriptor matching, BB-Homography uses bipartite graph matching (GM) algorithm to refine the matching results, which are finally passed over to estimate homography. On realizing the correlation between keypoint correspondence and homography estimation, BB-Homography iteratively performs the GM and the homography estimation such that they can refine each other at each iteration. In particular, based on spectral graph, a fast bipartite GM algorithm is developed for lowering the time cost of BB-Homography. BB-Homography is extensively evaluated on both public benchmarks and live-captured video streams that consistently shows that BB-Homography outperforms conventional methods for homography estimation.},
author = {Liu, Shaoguo and Wang, Haibo and Wei, Yiyi and Pan, Chunhong},
doi = {10.1109/TCSVT.2014.2339591},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/BB-Homography$\backslash$; Joint Binary Features and Bipartite Graph Matching for Homography Estimation.pdf:pdf},
issn = {10518215},
journal = {IEEE Transactions on Circuits and Systems for Video Technology},
keywords = {BB-Homography,binary feature descriptor,graph matching(GM),homography,sparse spectral GM},
number = {2},
pages = {239--250},
title = {{BB-homography: Joint binary features and bipartite graph matching for homography estimation}},
volume = {25},
year = {2015}
}
@article{Islam2015,
author = {Islam, Shafiqul and Liu, P. X. and {El Saddik}, Abdulmotaleb and Yang, Yubin B.},
doi = {10.1109/TMECH.2013.2297354},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/Bi.pdf:pdf},
isbn = {1083-4435 VO  - 20},
issn = {10834435},
journal = {IEEE/ASME Transactions on Mechatronics},
keywords = {Nonpassive input forces,teleoperation,time-varying delay},
number = {1},
pages = {1--12},
title = {{Bilateral control of teleoperation systems with time delay}},
volume = {20},
year = {2015}
}
@article{Calonder2010,
abstract = {We propose to use binary strings as an efficient feature point descriptor, which we call BRIEF. We show that it is highly discriminative even when using relatively few bits and can be computed using simple intensity difference tests. Furthermore, the descriptor similarity can be evaluated using the Hamming distance, which is very efficient to com-pute, instead of the L2 norm as is usually done. As a result, BRIEF is very fast both to build and to match. We compare it against SURF and U-SURF on standard benchmarks and show that it yields a similar or better recognition performance, while running in a fraction of the time required by either.},
author = {Calonder, Michael and Lepetit, Vincent and Strecha, Christoph and Fua, Pascal},
doi = {10.1007/978-3-642-15561-1_56},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/BRIEF.pdf:pdf},
isbn = {364215560X},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 4},
pages = {778--792},
pmid = {19500939},
title = {{BRIEF: Binary robust independent elementary features}},
volume = {6314 LNCS},
year = {2010}
}
@article{Endo1997,
author = {Endo, Kimitaka and The, Naoki},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/予見制御による視差軌道追従制御.pdf:pdf},
keywords = {optimal control,preview control,trajectory tracking,visual servoing},
title = {{by Preview}},
year = {1997}
}
@article{Cui2014,
abstract = {In this paper, a novel approach of SEM calibration based on non-linear minimization process is presented. The SEM calibration for the intrinsic parameters are achieved by an iterative non-linear optimization algorithm which minimize the registration error between the current estimated position of the pattern and its observed position. The calibration can be achieved by one image and multiple images of calibration pattern. Perspective and parallel projection models are addressed in this approach. The experimental results show the efficiency and accuracy of the proposed method.},
author = {Cui, Le and Marchand, Eric},
doi = {10.1109/ICRA.2014.6907621},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cui, Marchand - 2014 - Calibration of Scanning Electron Microscope using a multi-image non-linear minimization process.pdf:pdf},
isbn = {978-1-4799-3685-4},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {5191--5196},
title = {{Calibration of scanning electron microscope using a multi-image non-linear minimization process}},
year = {2014}
}
@article{Effendi2010,
abstract = {An intelligent robotic living assistive system has become a popular research in the last decade. One of the important topics in that research area is 3D object reconstruction from multiple views. This process may depend on motion estimation using vision. However, often a domestic robot on an electric wheel chair has to move in a steep rotational angle that causes motion estimation from vision to become inaccurate. In addition, an oblique viewing angle creates a perspective distortion to the captured images, which further worsens the estimation result. Hence, in this paper, we propose a new approach by altering the motion estimation problem into a 2D image registration problem. Our method{\&}{\#}x02019;s accuracy is very close to that of the Scale Invariant Feature Transform (SIFT) features tracker, whereas the Kanade-Lucas-Tomasi (KLT) tracker{\&}{\#}x02019;s drops as soon as the rotational angle reaches about 40?. Although our method is 2.7 times slower than the KLT tracker, it is 19 times faster than the SIFT tracker.},
author = {Effendi, Sutono and Jarvis, Ray},
doi = {10.1109/DICTA.2010.38},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Effendi, Jarvis - 2010 - Camera ego-motion estimation using phase correlation under planar motion constraint.pdf:pdf},
isbn = {9780769542713},
journal = {Proceedings - 2010 Digital Image Computing: Techniques and Applications, DICTA 2010},
pages = {158--165},
title = {{Camera ego-motion estimation using phase correlation under planar motion constraint}},
year = {2010}
}
@article{Delabarre2013,
abstract = {This paper deals with dense visual tracking robust towards scene perturbations using 3D information to provide a space-time coherency. The proposed method is based on an piecewise-planar scenes visual tracking algorithm which aims to minimize an error between an observed image and a reference template by estimating the parameters of a rigid 3D transformation taking into acount the relative positions of the planes in the scene. The major drawback of this approch stems from the registration function used to perform the minimization (the sum of squared differences) as it is very poorly robust towards scene variations. In this paper, the tracking process is adapted to take into account two more complex registration functions. First, the sum of conditional variance. Since it is invariant to global illumination variations, the proposed algorithm is robust with relation to those conditions whilst keeping a low computation complexity. Then, the mutual information is considered. In that case the complexity is greater but so is the robustness towards non global illumination variations, specularities or occlusions. The proposed approaches, after being described, are tested on different scenes under varying illumination conditions to assess their respective efficiency.},
author = {Delabarre, Bertrand and Marchand, Eric},
doi = {10.1109/IROS.2013.6696566},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Delabarre, Marchand - 2013 - Camera localization using mutual information-based multiplane tracking.pdf:pdf},
isbn = {9781467363587},
issn = {21530858},
journal = {IEEE International Conference on Intelligent Robots and Systems},
month = {nov},
pages = {1620--1625},
publisher = {Ieee},
title = {{Camera localization using mutual information-based multiplane tracking}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6696566},
year = {2013}
}
@inproceedings{Azad2009,
abstract = {In the recent past, the recognition and localization of objects based on local point features has become a widely accepted and utilized method. Among the most popular features are currently the SIFT features, the more recent SURF features, and region-based features such as the MSER. For time-critical application of object recognition and localization systems operating on such features, the SIFT features are too slow (500600ms for images of size 640480 on a 3GHz CPU). The faster SURF achieve a computation time of 150240 ms, which is still too slow for active tracking of objects or visual servoing applications. In this paper, we present a combination of the Harris corner detector and the SIFT descriptor, which computes features with a high repeatability and very good matching properties within approx. 20 ms. While just computing the SIFT descriptors for computed Harris interest points would lead to an approach that is not scale-invariant, we will show how scale-invariance can be achieved without a time-consuming scale space analysis. Furthermore, we will present results of successful application of the proposed features within our system for recognition and localization of textured objects. An extensive experimental evaluation proves the practical applicability of our approach.},
author = {Azad, Pedram and Asfour, Tamim and Dillmann, R{\"{u}}diger},
booktitle = {2009 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2009},
doi = {10.1109/IROS.2009.5354611},
file = {:C$\backslash$:/Users/yoshi/Dropbox/Combining Harris Interest Points and the SIFT Descriptor for Fast Scale Invariant Object Recognition.pdf:pdf},
isbn = {9781424438044},
pages = {4275--4280},
title = {{Combining Harris interest points and the SIFT descriptor for fast scale-invariant object recognition}},
year = {2009}
}
@article{Nigri2010,
author = {Nigri, Ilana and Meggiolaro, Marco Antonio and Feitosa, Raul Queiroz},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nigri, Meggiolaro, Feitosa - 2010 - COMPARISON BETWEEN LOOK-AND-MOVE AND VISUAL SERVO CONTROL USING SIFT TRANSFORMS IN EYE-IN-HAND.pdf:pdf},
journal = {Mechanical Engineering},
keywords = {eye-in-hand,look-and-move control,robotic manipulator,sift transform,visual servo control},
pages = {648--657},
title = {{Comparison Between Look-and-Move and Visual Servo Control Using Sift Transforms in Eye-in-Hand}},
volume = {4},
year = {2009}
}
@article{Kotek,
abstract = {Optical methods for measuring displacement is a key enabling technology allowing evaluate vibration of the light weighted objects. In optical evaluation of vibration of object displacement with high speed camera, systematical errors occurs in determining the displacement because of the tracking mark properties. The present article compares the results of optical tracking of four most common tracking marks (Filmmakin, Ridgeline creative, Eye fish and Blurry vision) and also own designed tracking mark (FMEIII). To verify the suitability of different tracking marks with high-speed camera, the test stand were constructed and measured data were compared to highspeed laser interferometer measurement.},
author = {Kotek, Lubo{\v{s}} and Holub, Michal and Veti{\v{s}}ka, Jan and {\v{S}}ubrt, Kamil and Hada{\v{s}}, Zden{\v{e}}k and Blecha, Petr},
doi = {10.1109/MECHATRONIKA.2014.7018308},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kotek et al. - Unknown - Comparison of Suitability of Tracking Marks for Optical Measurement of Displacement.pdf:pdf},
isbn = {9788021448162},
journal = {Proceedings of the 16th International Conference on Mechatronics, Mechatronika 2014},
keywords = {displacement,high-speed camera,tracking marks,vibration},
pages = {489--493},
title = {{Comparison of suitability of tracking marks for optical measurement of displacement}},
year = {2014}
}
@article{Muis2006,
abstract = {This paper proposes dual compliance controllers based on estimated torque and visual force on mobile manipulator to provide cooperativeness. A compliance control provides robot with affinity and adaptability to work in open environment. Generally, compliance control is a control system with trajectory compensation so that an external force may be followed. However, this paper compensates not only external force of physical sensation, but also virtual force of non-physical sensation based on visual information. Here, the robot has compliance with wider sensation. The visual information is provided by highspeed vision sensor. The target object is attached with hexagon pattern of artificial markers to reduce the required image processing time. The distance approximation between the object and the camera is used as visual force not as position or speed reference that commonly used in visual servoing. Later, the visual force adjusts the mobile manipulator command reference. Finally, the experimental result shows the validity of the proposed method},
author = {Muis, Abdul and Ohnishi, Kouhei},
doi = {10.1109/ICMECH.2006.252597},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Muis, Ohnishi - 2006 - Cooperative mobile manipulator with dual compliance controllers based on estimated torque and visual force.pdf:pdf},
isbn = {0780397134},
issn = {0913-6339},
journal = {2006 IEEE International Conference on Mechatronics, ICM},
pages = {619--624},
title = {{Cooperative mobile manipulator with dual compliance controllers based on estimated torque and visual force}},
year = {2006}
}
@article{Points2004,
abstract = {コンヒ゜ュータヒ゛シ゛ョン,特に三次元復元の研究は画像上の点や直線なと゛の幾何学的フ゜リミティフ゛に基つ゛いて 解析か゛行われる.このとき,実際の画像からそのような点や直線をと゛う抽出するかか゛問題となる.しかし,市 販の画像処理の教科書のほとんと゛は古典的な画像修復や輝度値変換を主とし,エッシ゛検出には触れているもの の,コンヒ゜ュータヒ゛シ゛ョンの中心となる特徴点に基つ゛く処理にほとんと゛触れていない.現実には Harris 作用素 と呼ふ゛特徴点抽出法か゛よく用いられるか゛,その原論文か゛入手しにくいこともあり,その原理についての誤解も 多い.そこて゛本稿て゛は,特徴点抽出の原理を Harris 作用素を中心にして解説する.},
author = {Kanazawa, Y. and Kanatani, K.},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Points, Vision - 2004 - Detection of Feature Points for Computer Vision A bstract.pdf:pdf},
journal = {The Journal of the Institute of Electronics, Information, and Communication Engineers},
keywords = {Harris作用素,computer vision,feature points,harris,コンピュータビジョン,局所相関,微分幾何学,特徴点},
number = {12},
pages = {1043--1048},
title = {{Detection of Feature Points for Computer Vision}},
volume = {87},
year = {2004}
}
@article{Oh2007,
abstract = {Machine vision systems become to play an important role in robot based factory automation, because they enable robots to recognize the environment around the workspace like human visual systems and make the manufacturing process flexible. So a large number of 2-dimensional (2D) vision systems have been successfully applied for industrial robots, but there are increasing needs for 3-dimensional (3D) vision systems providing accurate 3D position of work-pieces. For the 3D vision system, the stereo vision based position measurement systems have been applied in various fields. However the stereo vision systems which have been applied were specific for their particular tasks only. This paper introduces a general-purpose stereo vision system applicable to wide range of tasks with industrial robots.},
author = {Oh, Jong Kyu and Lee, Chan Ho},
doi = {10.1109/ICCAS.2007.4406981},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Oh, Lee - 2007 - Development of a stereo vision system for industrial robots.pdf:pdf},
isbn = {8995003871},
journal = {ICCAS 2007 - International Conference on Control, Automation and Systems},
keywords = {3D reconstruction,Camera calibration,Industrial robot,Stereo vision},
pages = {658--662},
title = {{Development of a stereo vision system for industrial robots}},
year = {2007}
}
@article{Silveira2012,
abstract = {This paper addresses the problem of stabilizing a robot at a pose specified via a reference image. Specifically, this paper focuses on six degrees-of-freedom visual servoing techniques that require neither metric information of the observed object nor precise camera and/or robot calibration parameters. Not requiring them improves the flexibility and robustness of servoing tasks. However, existing techniques within the focused class need prior knowledge of the object shape and/or of the camera motion. We present a new visual servoing technique that requires none of the aforementioned information. The proposed technique directly exploits 1) the projective parameters that relate the current image with the reference one and 2) the pixel intensities to obtain these parameters. The level of versatility and accuracy of servoing tasks are, thus, further improved. We also show that the proposed nonmetric scheme allows for path planning. In this way, the domain of convergence is greatly enlarged as well. Theoretical proofs and experimental results demonstrate that visual servoing can, indeed, be highly accurate and robust, despite unknown objects and imaging conditions. This naturally encompasses the cases of color images and illumination changes.},
author = {Silveira, Geraldo and Malis, Ezio},
doi = {10.1109/TRO.2012.2190875},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/Direct Visual Servoing Vision-Based Estimation and Control Using Only Nonmetric Information.pdf:pdf},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Computer vision,image registration,intensity-based methods,lighting variations,projective information,vision-based control},
number = {4},
pages = {974--980},
title = {{Direct visual servoing: Vision-based estimation and control using only nonmetric information}},
volume = {28},
year = {2012}
}
@article{Li2012,
abstract = {The drift distorts the atomic force microscopy (AFM) images as the time taken to acquire a complete AFM image is relatively long (a few minutes). As the AFM image is used as a reference for most manipulation mechanisms, the image distorted by drift will cause problems for AFM-based manipulation because the displayed positions of the objects under nanomanipulation do not match their actual locations. The drift during manipulation, similarly, will further exacerbate the mismatch between the displayed positions and the actual locations. Such mismatch is a major hurdle to achieve automation in AFM-based nanomanipulation. Without proper compensation, manipulation based on a wrong displayed location of the object often fails. In this paper, we present an algorithm to identify and eliminate the drift-induced distortion in the AFM image by applying a strategic local scan method. Briefly, after an AFM image is captured, the entire image is divided into several parts along vertical direction. A quick local scan is performed in each part of the image to measure the drift value in that very part. In this manner, the drift value is calculated in a small local area instead of the global image. Thus, the drift can be more precisely estimated and the actual position of the objects can be more accurately identified. In this paper, we also present the strategy to constantly compensate the drift during manipulation. By applying local scan on a single fixed feature in the AFM image frequently, the most current positions of all objects can be displayed in the augmented reality for real-time visual feedback. {\textcopyright} 2012 IEEE.},
author = {Li, Guangyong and Wang, Yucai and Liu, Lianqing},
doi = {10.1109/TASE.2012.2211077},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2012 - Drift Compensation in AFM-Based Nanomanipulation by Strategic Local Scan.pdf:pdf},
issn = {15455955},
journal = {IEEE Transactions on Automation Science and Engineering},
keywords = {Atomic force microscope (AFM),Drift compensation,Nanomanipulation},
number = {4},
pages = {755--762},
title = {{Drift compensation in AFM-based nanomanipulation by strategic local scan}},
volume = {9},
year = {2012}
}
@article{Nguyen2014,
abstract = {This paper describes a Kalman filter for a class of linear system in which: 1) The sampling time of measurement is longer than the control period. 2) The measurement is delayed. Using the delayed measurement and the present and past time predicted state, the pseudo-measurement in present time is constructed. The optimal Kalman gain is designed to minimize the trace of the covariance of estimation error. A dual rate recursive algorithm is proposed to estimate the state synchronously with the control signal. A visual servo system is demonstrated to verify the effectiveness of the proposed method.},
author = {Nguyen, Binh Minh and Ohnishi, Wataru and Wang, Yafei and Fujimoto, Hiroshi and Hori, Yoichi and Ito, Kiyoto and Odai, Masaki and Ogawa, Hironori and Takano, Erii and Inoue, Tomohiro and Koyama, Masahiro},
doi = {10.1109/AMC.2014.6823331},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nguyen, Ohnishi, Wang - 2014 - Dual Rate Kalman Filter Considering Delayed Measurement and Its Application in Visual Servo.pdf:pdf},
isbn = {9781479923243},
journal = {International Workshop on Advanced Motion Control, AMC},
keywords = {Kalman filter,visual servo},
pages = {494--499},
title = {{Dual rate Kalman filter considering delayed measurement and its application in visual servo}},
year = {2014}
}
@article{Ao,
author = {{\'{A}}{\`{o}}, {\`{O}} {\"{E}} {\'{Y}} {\O} {\~{N}} {\"{E}} {\`{O}} {\`{I}} {\'{O}} {\'{Y}} {\'{O}} and {\"{I}}{\'{o}}{\"{o}}, {\~{A}} {\'{Y}}},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/サンプル値制御理論３.pdf:pdf},
title = {{{\`{E}} ?? × {\`{e}} ?? × {\`{e}} ?? × {\`{e}} ?? × ´}}
}
@article{Mei2008,
abstract = {This paper addresses the problem of motion estimation and 3-D reconstruction through visual tracking with a single-viewpoint sensor and, in particular, how to generalize tracking to calibrated omnidirectional cameras. We analyze different minimization approaches for the intensity-based cost function (sum of squared differences). In particular, we propose novel variants of the efficient second-order minimization (ESM) with better computational complexities and compare these algorithms with the inverse composition (IC) and the hyperplane approximation (HA). Issues regarding the use of the IC and HA for 3-D tracking are discussed. We show that even though an iteration of ESM is computationally more expensive than an iteration of IC, the faster convergence rate makes it globally faster. The tracking algorithm was validated by using an omnidirectional sensor mounted on a mobile robot.},
author = {Mei, Christopher and Benhimane, Selim and Malis, Ezio and Rives, Patrick},
doi = {10.1109/TRO.2008.2007941},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/Efficient homography-based tracking and 3D reconstruction for single viewpoint sensors.pdf:pdf},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Omnidirectional vision,Structure from motion,Visual tracking},
number = {6},
pages = {1352--1364},
title = {{Efficient homography-based tracking and 3-D reconstruction for single-viewpoint sensors}},
volume = {24},
year = {2008}
}
@article{Motor2015,
abstract = {An approach to eliminate mutual flux effect on rotor position estimation of switched reluctance motor (SRM) drives at rotating shaft conditions without a prior knowledge of mutual flux is proposed in this paper. Neglecting the magnetic saturation, the operation of conventional self-inductance estimation using phase current slope difference method can be classified into three modes: Mode I, II and III. At positive-current-slope and negative-current-slope sampling point of one phase, the sign of current slope of the other phase changes in Mode I and II, but does not change in Mode III. Theoretically, based on characteristics of a 2.3 kW, 6000 rpm, three-phase 12/8 SRM, mutual flux introduces a maximum ±7{\%} self-inductance estimation error in Mode I and II, while, in Mode III, mutual flux effect does not exist. Therefore, in order to ensure that self-inductance estimation is working in Mode III exclusively, two methods are proposed: variable-hysteresis-band current control for the incoming phase and variable-sampling self-inductance estimation for the outgoing phase. Compared with the conventional method which neglects mutual flux effect, the proposed position estimation method demonstrates an improvement in position estimation accuracy by 2?. The simulations and experiments with the studied motor validate the effectiveness of the proposed method.},
author = {Ye, Jin and Bilgin, Berker and Emadi, Ali},
doi = {10.1109/TPEL.2014.2319238},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/06919380.pdf:pdf},
issn = {08858993},
journal = {IEEE Transactions on Power Electronics},
keywords = {Mutual flux,phase current slope difference,rotor position estimation,switched reluctance motor (SRM) drives},
number = {3},
pages = {1499--1512},
title = {{Elimination of mutual flux effect on rotor position estimation of switched reluctance motor drives}},
volume = {30},
year = {2015}
}
@article{Dame2009a,
author = {Dame, Amaury and Marchand, Eric},
doi = {10.1109/ROBOT.2009.5152492},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dame, Marchand - 2009 - Entropy-based visual servoing.pdf:pdf},
isbn = {978-1-4244-2788-8},
journal = {2009 IEEE International Conference on Robotics and Automation},
month = {may},
pages = {707--713},
publisher = {Ieee},
title = {{Entropy-based visual servoing}},
url = {http://ieeexplore.ieee.org/document/5152492/},
year = {2009}
}
@article{Watanabe,
author = {Watanabe, Sakiya and Fujimoto, Hiroshi},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Watanabe, Fujimoto - Unknown - Estimation of Sample Elasticity Utilizing Previous Line Surface Topography by AFM.pdf:pdf},
isbn = {9781479902231},
keywords = {Advanced Motion Control for Mechatronic Systems TC},
number = {10},
pages = {5--10},
title = {{Estimation of Sample Elasticity Utilizing Previous Line Surface Topography by AFM}},
year = {2013}
}
@article{Miksik2012,
abstract = {Local feature detectors and descriptors are widely used in many computer vision applications and various methods have been proposed during the past decade. There have been a number of evaluations focused on various aspects of local features, matching accuracy in particular, however there has been no comparisons considering the accuracy and speed trade-offs of recent extractors such as BRIEF, BRISK, ORB, MRRID, MROGH and LIOP. This paper provides a performance evaluation of recent feature detectors and compares their matching precision and speed in randomized kd-trees setup as well as an evaluation of binary descriptors with efficient computation of Hamming distance.},
author = {Miksik, Ondrej and Mikolajczyk, K.},
doi = {978-1-4673-2216-4},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/Evaluation of Local Detectors and Descriptors for Fast Feature Matching.pdf:pdf},
isbn = {9784990644109},
issn = {1051-4651},
journal = {Pattern Recognition (ICPR), 2012 21st International Conference on},
keywords = {Accuracy,Approximation methods,Artificial neural networks,BRIEF extractor,BRISK extractor,Databases,Detectors,Feature extraction,Hamming distance,LIOP extractor,MROGH extractor,MRRID extractor,ORB extractor,binary descriptor evaluation,computer vision,computer vision applications,feature detector performance evaluation,feature extraction,feature matching,image matching,local feature descriptor evaluation,local feature detector evaluation,matching accuracy,matching precision,matching speed,object detection,random processes,randomized kd-trees,tree data structures},
number = {Icpr},
pages = {2681--2684},
title = {{Evaluation of Local Detectors and Descriptors for Fast Feature Matching}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6460718},
year = {2012}
}
@article{Foroosh2002,
abstract = {In this paper, we have derived analytic expressions for the phase correlation of downsampled images. We have shown that for downsampled images the signal power in the phase correlation is not concentrated in a single peak, but rather in several coherent peaks mostly adjacent to each other. These coherent peaks correspond to the polyphase transform of a filtered unit impulse centered at the point of registration. The analytic results provide a closed-form solution to subpixel translation estimation, and are used for detailed error analysis. Excellent results have been obtained for subpixel translation estimation of images of different nature and across different spectral bands.},
author = {Foroosh, Hassan and Zerubia, Josiane B. and Berthod, Marc},
doi = {10.1109/83.988953},
file = {:C$\backslash$:/Users/yoshi/Dropbox/phase correlatio.pdf:pdf},
isbn = {1057-7149},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Image alignment,Phase correlation,Subpixel registration},
number = {3},
pages = {188--199},
pmid = {18244623},
title = {{Extension of phase correlation to subpixel registration}},
volume = {11},
year = {2002}
}
@article{Thesis2008,
author = {Thesis, Graduation},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thesis - 2008 - Feature Evaluation for Image Registration.pdf:pdf},
title = {{Feature Evaluation for Image Registration}},
year = {2008}
}
@article{Aranda2015,
abstract = {This paper describes a new vision-based control method to drive a set of robots moving on the ground plane to a desired formation. As the main contribution, we propose to use multiple camera-equipped unmanned aerial vehicles (UAVs) as control units. Each camera views, and is used to control, a subset of the ground team. Thus, the method is partially distributed, combining the simplicity of centralized schemes with the scalability and robustness of distributed strategies. Relying on a homography computed for each UAV-mounted camera, our approach is purely image-based and has low computational cost. In the control strategy we propose, if a robot is seen by multiple cameras, it computes its motion by combining the commands it receives. Then, if the intersections between the sets of robots viewed by the different cameras satisfy certain conditions, we formally guarantee the stabilization of the formation, considering unicycle robots. We also propose a distributed algorithm to control the camera motions that preserves these required overlaps, using communications. The effectiveness of the presented control scheme is illustrated via simulations and experiments with real robots.},
author = {Aranda, Miguel and Lopez-Nicolas, Gonzalo and Sagues, Carlos and Mezouar, Youcef},
doi = {10.1109/TRO.2015.2452777},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/Formation Control of Mobile Robots Using Multiple Aerial Cameras.pdf:pdf},
issn = {1552-3098},
journal = {IEEE Transactions on Robotics},
number = {4},
pages = {1064--1071},
title = {{Formation Control of Mobile Robots Using Multiple Aerial Cameras}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7163343},
volume = {31},
year = {2015}
}
@article{Shigeto,
abstract = {Lunar-planetary landers have landed on flat areas of surface, to avoid the areas where there are a lot of rocks, craters and hills on lunar-planetary surface. For the next generation exploration, lunar-planetary landers need tech-nologies to land on these areas to explore. In this paper, we propose predictive control for an active controlled landing gear with ballscrew linear actuator. The effectiveness of proposed method is shown by simulation and experimental results.},
author = {Shigeto, Shuhei and Fujimoto, Hiroshi and Hori, Yoichi and Otsuki, Masatsugu and Hashimoto, Tatsuaki},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shigeto, Fujimoto, Otsuki - Unknown - ISAS JAXA Fundamental Research on Reduction of Impact Forces for.pdf:pdf},
keywords = {(Active Landing gear,Ball Screw Actuator,Impact Force Reduction,Impedance Control ),Lunar Planetary Exploration},
number = {8},
pages = {2--7},
title = {{Fundamental Research on Reduction of Impact Forces for Active Controlled Landing Gear of Lunar-Planetary Lander}},
volume = {2}
}
@article{Pujiyoshi2007,
author = {Pujiyoshi, Hironobu},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pujiyoshi - 2007 - Gradient Based Feature Ext.pdf:pdf},
title = {{Gradient Based Feature Ext}},
year = {2007}
}
@article{Campbell,
author = {Campbell, N A and Wu, X},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/GRADIENT CROSS CORRELATION FOR SUB-PIXEL MATCHING.pdf:pdf},
journal = {The International Archives of the Photogrammetry: Remote Sensing and Spatial Information Sciences},
keywords = {correlation,fusion,matching,multisensor,registration},
number = {B7},
pages = {1065--1070},
title = {{Gradient cross correlation for sub-pixel matching}},
volume = {XXXVII},
year = {2008}
}
@article{I,
author = {Kobori, Norimasa and Deguchi, Daisuke and Takahashi, Tomokazu and Ide, Ichiro and Murase, Hiroshi},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/カメラとジャイロセンサを組み合わせた移動ロボットの高精度位置推定.pdf:pdf},
journal = {画像の認識・理解シンポジウム (Miru) 2009 論文集},
pages = {1757--1763},
title = {{High Accuracy Pose Estimation for a Mobile Robot fusing a Camera and Gyro Sensors}},
year = {2009}
}
@inproceedings{Takita2003,
abstract = {This paper presents a high-accuracy image reg- istration technique using a Phase-Only Correlation (POC) func- tion. Conventional techniques of phase-based image registration employ heuristic methods in estimating the location of the cor- relation peak, which corresponds to image displacement. This paper proposes a technique to improve registration performance by fitting the closed-form analytical model of the correlation peak to actual two-dimensional numerical data. This method can also be extended to a spectrum weighting POC technique, where we modify cross-phase spectrum with some weighting functions to enhance registration accuracy. The proposed method makes pos- sible to estimate image displacements with 1/100-pixel accuracy.},
author = {Takita, Kenji and Aoki, Takafumi and Sasaki, Yoshifumi and Higuchi, Tatsuo and Kobayashi, Koji},
booktitle = {IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Takita et al. - 2003 - High-Accuracy Subpixel Image Registration Based on Phase-Only Correlation.pdf:pdf},
issn = {09168508},
keywords = {Image matching,Image registration,Phase correlation,Phase-only correlation,Subpixel registration},
number = {8},
pages = {1925--1934},
title = {{High-Accuracy Subpixel Image Registration Based on Phase-Only Correlation}},
volume = {E86-A},
year = {2003}
}
@article{Tamadazte2011,
abstract = {This paper demonstrates accurate micropositioning scheme based on a direct visual servoing process. This technique uses only the pure image signal (photometric information) to design the control law. With respect to traditional visual servoing approaches that use geometric visual features (points, lines {\&}{\#}x2026;), the visual features used in the control law are the pixel intensity. The proposed approach was tested in term of accuracy and robustness in several experimental conditions. The obtained results have demonstrated a good behavior of the control law and very good positioning accuracy. The obtained accuracies are estimated to 14 nm, 89 nm, and 0.001 degrees in the x, y and {\&}{\#}x03B8; axes of positioning platform, respectively},
author = {Tamadazte, B. and Duceux, G. and {Le-Fort Piat}, N. and Marchand, E.},
doi = {10.1109/ICRA.2011.5979926},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/Highly precise micropositioning task using a direct visual servoing scheme.pdf:pdf},
isbn = {9781612843865},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {5689--5694},
title = {{Highly precise micropositioning task using a direct visual servoing scheme}},
year = {2011}
}
@article{Tamadazte2011a,
abstract = {This paper demonstrates accurate micropositioning scheme based on a direct visual servoing process. This technique uses only the pure image signal (photometric information) to design the control law. With respect to traditional visual servoing approaches that use geometric visual features (points, lines {\&}{\#}x2026;), the visual features used in the control law are the pixel intensity. The proposed approach was tested in term of accuracy and robustness in several experimental conditions. The obtained results have demonstrated a good behavior of the control law and very good positioning accuracy. The obtained accuracies are estimated to 14 nm, 89 nm, and 0.001 degrees in the x, y and {\&}{\#}x03B8; axes of positioning platform, respectively},
author = {Tamadazte, B. and Duceux, G. and {Le-Fort Piat}, N. and Marchand, E.},
doi = {10.1109/ICRA.2011.5979926},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tamadazte et al. - 2011 - Highly precise micropositioning task using a direct visual servoing scheme.pdf:pdf},
isbn = {9781612843865},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
month = {may},
pages = {5689--5694},
publisher = {Ieee},
title = {{Highly precise micropositioning task using a direct visual servoing scheme}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5979926},
year = {2011}
}
@article{,
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - High-Speed Computer Vision System for Robots.pdf:pdf},
pages = {29--32},
title = {{High-Speed Computer Vision System for Robots}}
}
@article{Tamada2013,
author = {Tamada, Tomoki and Yamakawa, Yuji and Senoo, Taku and Ishikawa, Masatoshi},
doi = {10.1109/ROBIO.2013.6739695},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tamada et al. - 2013 - High-Speed Manipulation of Cable Connector Using a High-Speed Robot Hand.pdf:pdf},
isbn = {9781479927449},
journal = {2013 IEEE International Conference on Robotics and Biomimetics, ROBIO 2013},
number = {December},
pages = {1598--1604},
title = {{High-speed manipulation of cable connector using a high-speed robot hand}},
year = {2013}
}
@article{Benhimane2006,
abstract = {The objective of this paper is to propose a new homography-based approach$\backslash$nto image-based visual tracking and servoing. The visual tracking$\backslash$nalgorithm proposed in the paper is based on a new efficient second-order$\backslash$nminimization method. Theoretical analysis and comparative experiments$\backslash$nwith other tracking approaches show that the proposed method has$\backslash$na higher convergence rate than standard first-order minimization$\backslash$ntechniques. Therefore, it is well adapted to real-time robotic applications.$\backslash$nThe output of the visual tracking is a homography linking the current$\backslash$nand the reference image of a planar target. Using the homography,$\backslash$na task function isomorphic to the camera pose has been designed.$\backslash$nA new image-based control law is proposed which does not need any$\backslash$nmeasure of the 3D structure of the observed target (e.g. the normal$\backslash$nto the plane). The theoretical proof of the existence of the isomorphism$\backslash$nbetween the task function and the camera pose and the theoretical$\backslash$nproof of the stability of the control law are provided. The experimental$\backslash$nresults, obtained with a 6 d.o.f. robot, show the advantages of the$\backslash$nproposed method with respect to the existing approaches.},
author = {Benhimane, Selim and Malis, Ezio},
doi = {10.1109/ROBOT.2006.1642061},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Benhimane, Malis - 2006 - Homography-based 2D visual servoing.pdf:pdf},
isbn = {0780395069},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
number = {May},
pages = {2397--2402},
title = {{Homography-based 2D visual servoing}},
volume = {2006},
year = {2006}
}
@article{Malis2007,
abstract = {The objective of this paper is to propose a new homography-based approach$\backslash$nto image-based visual tracking and servoing. The visual tracking$\backslash$nalgorithm proposed in the paper is based on a new efficient second-order$\backslash$nminimization method. Theoretical analysis and comparative experiments$\backslash$nwith other tracking approaches show that the proposed method has$\backslash$na higher convergence rate than standard first-order minimization$\backslash$ntechniques. Therefore, it is well adapted to real-time robotic applications.$\backslash$nThe output of the visual tracking is a homography linking the current$\backslash$nand the reference image of a planar target. Using the homography,$\backslash$na task function isomorphic to the camera pose has been designed.$\backslash$nA new image-based control law is proposed which does not need any$\backslash$nmeasure of the 3D structure of the observed target (e.g. the normal$\backslash$nto the plane). The theoretical proof of the existence of the isomorphism$\backslash$nbetween the task function and the camera pose and the theoretical$\backslash$nproof of the stability of the control law are provided. The experimental$\backslash$nresults, obtained with a 6 d.o.f. robot, show the advantages of the$\backslash$nproposed method with respect to the existing approaches.},
author = {Malis, E and Benhimane, S},
doi = {10.1177/0278364907080252},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Malis, Benhimane - 2007 - Homography-based 2D Visual Tracking and Servoing.pdf:pdf},
isbn = {0278-3649},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {efficient,homography-based control law,second-order minimization,visual servoing,visual tracking},
number = {7},
pages = {661--676},
title = {{Homography-based 2D Visual Tracking and Servoing}},
volume = {26},
year = {2007}
}
@article{Salgado2010,
abstract = {This study presents a robust method for ground plane detection in vision-based systems with a non-stationary camera. The proposed method is based on the reliable estimation of the homography between ground planes in successive images. This homography is computed using a feature matching approach, which in contrast to classical approaches to on-board motion estimation does not require explicit ego-motion calculation. As opposed to it, a novel homography calculation method based on a linear estimation framework is presented. This framework provides predictions of the ground plane transformation matrix that are dynamically updated with new measurements. The method is specially suited for challenging environments, in particular traffic scenarios, in which the information is scarce and the homography computed from the images is usually inaccurate or erroneous. The proposed estimation framework is able to remove erroneous measurements and to correct those that are inaccurate, hence producing a reliable homography estimate at each instant. It is based on the evaluation of the difference between the predicted and the observed transformations, measured according to the spectral norm of the associated matrix of differences. Moreover, an example is provided on how to use the information extracted from ground plane estimation to achieve object detection and tracking. The method has been successfully demonstrated for the detection of moving vehicles in traffic environments.},
author = {Arrospide, J and Salgado, L and Nieto, M and Mohedano, R},
doi = {10.1049/iet-its.2009.0073},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/Homography-based ground plane detection using a single on-board camera.pdf:pdf},
isbn = {1751-956X},
issn = {1751956X},
journal = {Intelligent Transport Systems, IET},
keywords = {cameras,ego-motion calculation,erroneous measurement removal,feature matching approach,ground plane estimation,ground plane transformation matrix,homography calculation method,homography-based ground plane detection,image matching,linear estimation framework,motion estimation,moving vehicles detection,nonstationary camera,object detection,object tracking,onboard motion estimation,single onboard camera,traffic environments},
number = {2},
pages = {149--160},
title = {{Homography-based ground plane detection using a single on-board camera}},
volume = {4},
year = {2010}
}
@article{Goncalves2010,
abstract = {This paper proposes the Euclidean homography matrix as visual feature in an image-based visual servoing scheme in order to control an aircraft along the approach and landing phase. With a trajectory defined in the image space by a sequence of equidistant key images along the glidepath, an interpolation in the homography space is also proposed in order to reduce the database size and ensure the required smoothness of the control task. In addition, a pan-tilt control was taken into account to respect the dynamics of the aircraft during manoeuvres and in the presence of wind perturbations. An optimal control design based on the linearized model of the aircraft dynamics is then consider to cancel the visual error function. To demonstrate the proposed concept, simulation results under realistic atmospheric disturbances are presented.},
author = {Gon{\c{c}}alves, Tiago and Azinheira, Jos{\'{e}} and Rives, Patrick},
doi = {10.1109/ROBOT.2010.5509496},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gon{\c{c}}alves, Azinheira, Rives - 2010 - Homography-based visual servoing of an aircraft for automatic approach and landing.pdf:pdf},
isbn = {9781424450381},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {9--14},
title = {{Homography-based visual servoing of an aircraft for automatic approach and landing}},
year = {2010}
}
@article{,
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/サンプル値制御理論V.pdf:pdf},
title = {{{\"{I}} × ? {\"{i}} × ? ,}}
}
@article{Aoki2007,
author = {Aoki, Takafumi and Sciences, Information and Ito, Koichi and Shibahara, Takuma and Nagashima, Sei},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Aoki et al. - 2007 - i ii Kuglin MACE Minimum Average Correlation Energy correlation phase correlation phase-only matched filtering DFT.pdf:pdf},
journal = {Review Literature And Arts Of The Americas},
number = {1},
pages = {30--40},
title = {{i ii Kuglin MACE Minimum Average Correlation Energy correlation phase correlation phase-only matched filtering DFT : Discrete Fourier Transform POC Discrete Fourier Transform IDFT : Inverse}},
volume = {1},
year = {2007}
}
@article{Ma2009,
abstract = {Image matching is an important topic in the field of image processing, and it is widely used in image registration and image fusion. An image fast template matching algorithm based on projection and sequential similarity detecting is proposed. The algorithm employs the strategy of image template matching that is from sketch matching to detail matching. Firstly the real-time images are projected to obtain one dimension data, and it is employed for sketch matching with one dimension data from reference image. Secondly sequential similarity detecting principle is used for the detail matching employing the points with larger similarity in the sketch matching. The experimental results showed that the proposed algorithm was efficient and faster than conventional image template matching algorithms.},
author = {Ma, Liyong and Sun, Yude and Feng, Naizhang and Liu, Zheng},
doi = {10.1109/IIH-MSP.2009.94},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/Image Fast Template Matching Algorithm Based on Projection and Sequential Similarity Detecting.pdf:pdf},
isbn = {9780769537627},
journal = {IIH-MSP 2009 - 2009 5th International Conference on Intelligent Information Hiding and Multimedia Signal Processing},
keywords = {Image template matching,Projection,Sequential similarity detecting},
pages = {957--960},
title = {{Image fast template matching algorithm based on projection and sequential similarity detecting}},
year = {2009}
}
@article{Ojansivu2007,
author = {Ojansivu, Ville},
file = {:C$\backslash$:/Users/yoshi/Pictures/forBlog/Image registration Using Blur-Invariant pahse Correlation.pdf:pdf},
number = {7},
pages = {449--452},
title = {{Image Registration Using Blur-Invariant}},
volume = {14},
year = {2007}
}
@article{Sciences,
author = {MIYAZAWA, Kazuyuki},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sciences - Unknown - Image-Based Human Body Measurement Using Phase-Only Correlation and Its Applications.pdf:pdf},
title = {{Image-Based Human Body Measurement Using Phase-Only Correlation and Its Applications}},
year = {2010}
}
@article{Dame2010a,
author = {Dame, A and Marchand, E},
doi = {10.1109/ROBOT.2010.5509540},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dame, Marchand - 2010 - Improving mutual information-based visual servoing.pdf:pdf},
isbn = {978-1-4244-5038-1},
journal = {IEEE Int. Conf. on Robotics and Automation, ICRA'10},
month = {may},
pages = {5531--5536},
publisher = {Ieee},
title = {{Improving mutual information based visual servoing}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5509540},
year = {2010}
}
@article{2002,
abstract = {We examined whether configural information was used in face detection. Facial parts were detected more quickly when they were presented in the context of faces than when they were presented alone (Experiment 1). The facilitation effect was also found for inverted faces (Experiment 2). However, the context of houses did not facilitate the detection of windonw (Experiment 1). These resluts suggested that configural information was used in face detection, but not in other basic-level object recognition.},
author = {{栗原 淳} and {清本 晋作} and {福島 和英} and {田中 俊昭}},
doi = {10.1080/01411920600775449},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - The Institute of Electronics, Information and Communication Engineers NII-Electronic Library Service.pdf:pdf},
isbn = {0025190900251909},
issn = {09135685},
journal = {Library},
number = {209},
pages = {25--30},
title = {{Institute of Electronics, Information, and Communication Engineers NII-Electronic Library Service}},
url = {http://ci.nii.ac.jp/naid/110003498100/},
volume = {107},
year = {2002}
}
@article{,
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Institute of Systems, Control and Information Engineers NII-Electronic Library Service(4).pdf:pdf},
journal = {move to Dropbox},
title = {{Institute of Systems, Control and Information Engineers NII-Electronic Library Service}}
}
@article{,
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Institute of Systems, Control and Information Engineers NII-Electronic Library Service(2).pdf:pdf},
journal = {move to Dropbox},
title = {{Institute of Systems, Control and Information Engineers NII-Electronic Library Service}}
}
@misc{Zhao2001,
author = {Zhao, He},
booktitle = {Artificial Intelligence},
number = {x},
pages = {3--6},
title = {{Inverse Compositional Method}},
url = {http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL{\_}COPIES/AV0910/zhao.pdf},
urldate = {2016-02-02},
volume = {2},
year = {2001}
}
@article{Odile2007,
author = {Odile, Bourquardez and Mahony, Robert and Guenard, Nicolas and Chaumette, Fran{\c{c}}ois and Hamel, Tarek and Eck, Laurent},
file = {:C$\backslash$:/Users/yoshi/Dropbox/05068838.pdf:pdf},
number = {3},
pages = {833--838},
title = {{Kinematic Visual Servo Control of a Quadrotor Aerial Vehicle}},
volume = {25},
year = {2007}
}
@article{Servoing1995,
author = {Servoing, Visual and Approach, Nonlinear Control},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/ビジュアルサーボイング非線形制御.pdf:pdf},
title = {{Koichi Takahiro and Hidenori Kimura * 3}},
year = {1995}
}
@article{Kamado,
author = {Kamado, Mitsuhiko},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kamado - Unknown - Legged robot control based on visual infomation ? Experiment on actual robot ?.pdf:pdf},
number = {2},
pages = {8--9},
title = {{Legged robot control based on visual infomation ? Experiment on actual robot ?}}
}
@article{Maki2011,
abstract = {This paper describes a method of detecting and tracking of a specific accelerometer in a camera view. The proposed method extracts and tracks local feature points from an image sequence and assign a score, which becomes larger when it is more likely to be associated with the accelerometer motion, to each feature point. By considering these scores as random samples from he probability density function of the existence of the accelerometer over the image, we estimate its position. Experimental results show the effectiveness of the proposed method.},
author = {Maki, Y and Kagami, S and Hashimoto, K},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/画像特徴点運動に基づくカメラ視野内の加速度センサの位置推定と追跡.pdf:pdf},
isbn = {9781467322591},
journal = {SICE Annual Conference ( {\ldots}},
pages = {293 -- 294},
title = {{Localization and tracking of an accelerometer in a camera view based on feature point motion analysis}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6318450},
volume = {4},
year = {2012}
}
@article{Baker2003,
abstract = {citation: 1540 (2014/07/05)},
author = {Baker, Simon and Matthews, Iain},
doi = {10.1023/B:VISI.0000011205.11775.fd},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Baker, Gross - 2003 - Lucas-Kanade 20 years on a unifying framework .pdf:pdf},
isbn = {0920-5691},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {A unifying framework,Additive vs. compositional algorithms,Efficiency,Forwards vs. inverse algorithms,Gauss-Newton,Image alignment,Levenberg-Marquardt,Lucas-Kanade,Newton,Steepest descent,The inverse compositional algorithm},
number = {3},
pages = {221--255},
title = {{Lucas-Kanade 20 years on: A unifying framework}},
url = {http://www.springerlink.com/openurl.asp?id=doi:10.1023/B:VISI.0000011205.11775.fd},
volume = {56},
year = {2004}
}
@article{Hashimoto1991,
abstract = {A visual feedback control scheme, called image-based visual servo,$\backslash$nis proposed for manipulators with cameras on their hands. To accomplish$\backslash$nspecific tasks in unstructured environments, it is essential to have$\backslash$ncapabilities of recognizing the object position with respect to the$\backslash$nmanipulator hand. Using the Jacobian matrix relating the camera motion$\backslash$nto the object position change in the acquired image, a state space$\backslash$nformulation of a visual feedback system is established. On the basis of$\backslash$nthe task defined in the image plane, the desired motion of the hand is$\backslash$nachieved by time-variant state feedback. The image-based scheme is$\backslash$napplied to a task of tracking a moving object by the camera (and the$\backslash$nhand). Simulations show that the control scheme is stable even under$\backslash$nnoisy conditions, while the conventional position-based servo tends to$\backslash$nbe unstable. Experiments on a PUMA 560 show the validity and the$\backslash$neffectiveness of the image-based visual servo},
author = {Hashimoto, K. and Kimoto, T. and Ebine, T. and Kimura, H.},
doi = {10.1109/ROBOT.1991.131968},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hashimoto et al. - 1991 - Manipulator Control with Image-Based Visual Servo.pdf:pdf},
isbn = {0-8186-2163-X},
issn = {10504729},
journal = {Proceedings. 1991 IEEE International Conference on Robotics and Automation},
number = {April},
pages = {2267--2272},
title = {{Manipulator control with image-based visual servo}},
year = {1991}
}
@misc{Malzahn2010,
abstract = {This contribution proposes a model free energy based oscillation damping approach applied to a three degree of freedom (3-DOF) robot arm. The arm experiences gravitational forces and exhibits large deflections. A camera mounted in eye-in-hand configuration provides the endpoint oscillation feedback signal computed from the motion of image features extracted from natural texture in the environment. Sparse features are tracked between two consecutive views. Their mean displacement and the reconstructed camera orientation rate of change constitutes the oscillation feedback signal. The feasibility and limits of image based oscillation and motion detection are discussed in detail. The proposed endeffector vibration damping techniques are analyzed in simulations and further validated in realistic experiments.},
author = {Malzahn, Joern and Phung, Anh Son and Franke, Rene and Hoffmann, Frank and Bertram, Torsten},
booktitle = {proceedings of 41st International Symposium on Robotics (ISR) and 6th German Conference on Robotics (ROBOTIK)},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Malzahn et al. - 2010 - Markerless Visual Vibration Damping of a 3-DOF Flexible Link Robot Arm.pdf:pdf},
isbn = {9783800732739},
keywords = {Adaptive optics,Cameras,Damping,Joints,Optical imaging,Oscillators,Robots},
pages = {401--408},
title = {{Markerless Visual Vibration Damping of a 3-DOF Flexible Link Robot Arm}},
year = {2010}
}
@article{,
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2007 - Matlab expo 2007 19 11 28.pdf:pdf},
title = {{Matlab expo 2007 19 11 28}},
year = {2007}
}
@article{Kyrki2006,
abstract = {This paper addresses the issue of measurement errors in visual servoing. The error characteristics of the vision based state estimation and the associated uncertainty of the control are investigated. The major contribution is the analysis of the propagation of image error through pose estimation and visual servoing control law. Using the analysis, two classical visual servoing methods are evaluated: position-based and 2.5D visual servoing. The evaluation offers a tool to build and analyze hybrid control systems such as switching or partitioning control. ?? 2006 Elsevier Ltd. All rights reserved.},
author = {Kyrki, V. and Kragic, D. and Christensen, H. I.},
doi = {10.1016/j.robot.2006.05.002},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/Measurement errors in visual servoing.pdf:pdf},
isbn = {0921-8890},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Error estimation,Measurement errors,Visual servoing},
number = {10},
pages = {815--827},
title = {{Measurement errors in visual servoing}},
volume = {54},
year = {2006}
}
@article{Kelly2006,
abstract = {This paper addresses the visual servoing for regulation of robot manipulators evolving in the 3D Cartesian space. The image-based approach using a single camera - but multiple feature points - is considered to propose a direct visual servoing to deal with the complete robot nonlinear dynamics. This paper exploits first the well-known fact that in general four coplanar feature points on an object are sufficient to determine its posture form their projection in the image plane, and second, the transpose Jacobian control technique to design the proposed direct visual servoing scheme. Experiments on a nonlinear direct-drive spherical wrist are presented to illustrate the effectiveness of the proposed method},
author = {Kelly, Rafael and Bugarin, Eusebio and Cervantes, Ilse and Alvarez-Ramirez, Jose},
doi = {10.1109/CDC.2006.377597},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/Monocular direct visual servoing for regulation of manipulators moving in the 3D Cartesian space.pdf:pdf},
isbn = {1-4244-0171-2},
issn = {01912216},
journal = {Proceedings of the 45th IEEE Conference on Decision and Control},
keywords = {Monocular,Robot manipulators,Stability,Transpose Jacobian,Visual servoing},
pages = {1782--1787},
title = {{Monocular direct visual servoing for regulation of manipulators moving in the 3D Cartesian space}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4177190},
year = {2006}
}
@article{Maes1997,
abstract = {This paper presents a novel image similarity measure, referred to as quantitative-qualitative measure of mutual information (Q-MI), for multimodality image registration. Conventional information measures, e.g., Shannon's entropy and mutual information (MI), reflect quantitative aspects of information because they only consider probabilities of events. In fact, each event has its own utility to the fulfillment of the underlying goal, which can be independent of its probability of occurrence. Thus, it is important to consider both quantitative (i.e., probability) and qualitative (i.e., utility) measures of information in order to fully capture the characteristics of events. Accordingly, in multimodality image registration, Q-MI should be used to integrate the information obtained from both the image intensity distributions and the utilities of voxels in the images. Different voxels can have different utilities, for example, in brain images, two voxels can have the same intensity value, but their utilities can be different, e.g., a white matter (WM) voxel near the cortex can have higher utility than a WM voxel inside a large uniform WM region. In Q-MI, the utility of each voxel in an image can be determined according to the regional saliency value calculated from the scale-space map of this image. Since the voxels with higher utility values (or saliency values) contribute more in measuring Q-MI of the two images, the Q-MI-based registration method is much more robust, compared to conventional MI-based registration methods. Also, the Q-MI-based registration method can provide a smoother registration function with a relatively larger capture range. In this paper, the proposed Q-MI has been validated and applied to the rigid registrations of clinical brain images, such as MR, CT and PET images. {\textcopyright} 2007 Pattern Recognition Society.},
author = {Luan, Hongxia and Qi, Feihu and Xue, Zhong and Chen, Liya and Shen, Dinggang},
doi = {10.1016/j.patcog.2007.04.002},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Maes et al. - 1997 - Multimodality image registration by maximization of mutual information.pdf:pdf},
isbn = {0278-0062 (Print)$\backslash$r0278-0062 (Linking)},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Multimodality image registration,Mutual information,Quantitative-qualitative measure of mutual information,Salient measure,Utilities of events},
month = {apr},
number = {1},
pages = {285--298},
pmid = {9101328},
title = {{Multimodality image registration by maximization of quantitative-qualitative measure of mutual information}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/9101328},
volume = {41},
year = {2008}
}
@article{Ducke2011,
abstract = {Multiview (n-view or multiple view) 3D reconstruction is the computationally complex process by which a full 3D model is derived from a series of overlapping images. It is based on research in the field of computer vision, which in turn relies on older methods from photogrammetry. This report presents a multiview reconstruction tool chain composed from various freely available, open source components and a practical application example in the form of a 3D model of an archaeological site. ?? 2011 Elsevier Ltd. All rights reserved.},
author = {Ducke, Benjamin and Score, David and Reeves, Joseph},
doi = {10.1016/j.cag.2011.01.006},
isbn = {0097-8493},
issn = {00978493},
journal = {Computers and Graphics (Pergamon)},
keywords = {3D reconstruction,Archaeology,Computer vision,Open source,Photogrammetry,Weymouth},
number = {2},
pages = {375--382},
title = {{Multiview 3D reconstruction of the archaeological site at Weymouth from image series}},
volume = {35},
year = {2011}
}
@article{Zhao2011,
author = {Zhao, Jianguo and Song, Bo and Xi, Ning and Wai, King and Lai, Chiu},
doi = {10.1109/CDC.2011.6161488},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao et al. - 2011 - Mutation analysis models for visual servoing in nanomanipulations.pdf:pdf},
isbn = {978-1-61284-801-3},
journal = {IEEE Conference on Decision and Control and European Control Conference},
month = {dec},
pages = {5683--5688},
publisher = {Ieee},
title = {{Mutation analysis models for visual servoing in nanomanipulations}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6161488},
year = {2011}
}
@article{Dame2011a,
author = {Dame, Amaury and Marchand, Eric},
doi = {10.1109/TRO.2011.2147090},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dame, Marchand - 2011 - Mutual Information-Based Visual Servoing.pdf:pdf},
issn = {1552-3098},
journal = {IEEE Transactions on Robotics},
month = {oct},
number = {5},
pages = {958--969},
title = {{Mutual Information-Based Visual Servoing}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5772938},
volume = {27},
year = {2011}
}
@article{Ishibashi,
author = {{石橋 央成} and {藤本 博志} and 石橋, 央成 and 藤本, 博志},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ishibashi - Unknown - Force Sensorless Control of Cutting Resistance for NC Machine Tools by Spindle Motor Control Utilizing Variable Pu.pdf:pdf},
journal = {電気学会研究会資料. Iic, 産業計測制御研究会},
keywords = {NC machine tools,NC工作機械,cutting resistance,disturbance observer,end milling,spindle speed control,variable pulse number T-method,エンドミル加工,主軸角速度制御,切削抵抗,可変パルス数計時法,外乱オブザーバ},
number = {17-32},
pages = {67--72},
title = {{Nc工作機械における可変パルス数計時法を用いた主軸モータによる力センサレス切削抵抗制御}},
url = {http://ci.nii.ac.jp/naid/10031160876},
volume = {2013},
year = {2013}
}
@article{,
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/カメラのキャリブレーション＿最小二乗法について.pdf:pdf},
title = {{No Title}}
}
@article{,
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/sotsuron{\_}ohnishi.pdf:pdf},
title = {{No Title}}
}
@article{Pole2013,
author = {Pole, Limited and Method, Placement and Calculation, Considering},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/ディジタル制御における演算時間遅れを考慮した限定極配置法の研究.pdf:pdf},
title = {{No Title}},
volume = {2013},
year = {2013}
}
@article{,
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2001 - No Title(2).pdf:pdf},
title = {{No Title}},
year = {2001}
}
@article{,
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/syuron{\_}watanabe.pdf:pdf},
title = {{No Title}}
}
@article{,
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2001 - No Title.pdf:pdf},
title = {{No Title}},
year = {2001}
}
@article{,
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/1000fps高速度画像処理のアクティブカメラへの応用.pdf:pdf},
number = {3},
title = {{No Title}},
year = {2005}
}
@article{,
title = {{No Title}}
}
@article{,
title = {{No Title}}
}
@article{,
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - No Title(3).pdf:pdf},
title = {{No Title}}
}
@article{,
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - No Title(4).pdf:pdf},
title = {{No Title}}
}
@article{,
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/ビジ ュアル サ ー ボ イ ング― 最 適 制 御 に よる ア プ ロー チ.pdf:pdf},
keywords = {servomechanisms,visual feedback},
title = {{No Title}}
}
@article{,
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - No Title(2).pdf:pdf},
number = {1},
pages = {1--6},
title = {{No Title}},
volume = {1}
}
@article{,
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2008 - No Title.pdf:pdf},
title = {{No Title}},
year = {2008}
}
@article{Carlo2009,
author = {Carlo, Monte},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/C言語による非線形カーブフィッティング.pdf:pdf},
number = {5},
pages = {1--13},
title = {{Non-linear Carve Fitting in C ++}},
year = {2009}
}
@article{Silveira2014,
abstract = {This paper considers the problem of vision-based robot stabilization where the equilibrium state is defined via a reference image. Differently from most solutions, this study directly exploits the pixel intensities with no feature extraction or matching and uses only nonmetric information of the observed scene. Intensity-based techniques provide higher accuracy, whereas not requiring metric information increases their versatility. In this context, this paper further exploits the epipolar geometry and its intrinsic degeneracies. Such degeneracies always occur when that stabilization is sufficiently close to the equilibrium, regardless of the object shape. This remarkable fact allows the development of new vision-based control strategies with varying degrees of computational complexity and of prior knowledge. Importantly, they are arranged hierarchically from the simplest to the state-of-the-art ones, all in a unified framework. Three new local methods are then presented, and their closed-loop performances are experimentally assessed using both planar and nonplanar objects, under small and large displacements, simulating and employing a six-degree-of-freedom robotic arm.},
author = {Silveira, Geraldo},
doi = {10.1109/TRO.2014.2315712},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/On Intensity-Based Nonmetric Visual Servoing.pdf:pdf},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Direct methods,epipolar geometry,image registration,vision-based control,vision-based estimation,visual servo control},
number = {4},
pages = {1019--1026},
title = {{On intensity-based nonmetric visual servoing}},
volume = {30},
year = {2014}
}
@article{Sattar2006,
abstract = {We consider the use of visual target tracking for autonomous steering of an underwater robot. In this context, we consider a performance comparison for three key visual tracking algorithms used for servo control. We present a comparative study of the performance in underwater environments of three tracking algorithms that are widely used in vision applications. Variations in illumination, suspended particles and a resulting reduction in visibility hinders vision systems from performing satisfactorily in marine environments; at least not as well as they do in terrestrial (Le. non-underwater) surroundings. Our work focuses on quantitatively measuring the performance of three color-based tracking algorithms- color blob tracker, color histogram tracker and mean-shift tracker, in tracking objects underwater in different levels lighting and visibility. We also present results demonstrating the effect of suspended particles underwater, and in conclusion we summarize the three tracking algorithms by comparing their pros and cons},
author = {Sattar, Junaed and Dudek, Gregory},
doi = {10.1109/ROBOT.2006.1642244},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/On the Performance of Color Tracking Algorithms for Underwater Robots under Varying Lighting and Visibility.pdf:pdf},
isbn = {0780395069},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
number = {May},
pages = {3550--3555},
title = {{On the performance of color tracking algorithms for underwater robots under varying lighting and visibility}},
volume = {2006},
year = {2006}
}
@article{Qian2002,
abstract = { This paper studies the visual servoing problem with sensory feedback from uncalibrated stereo cameras. The linear image Jacobian matrix is used to describe the spatial and temporary approximation of the differential movement relation between image space and robotic workspace. We suggest to construct an instrumental dynamic system with state variables formed from elements of the image Jacobian matrix. Thus a Kalman-Bucy filter is used to estimate the state variables of the constructed system online, which is robust to system noise and external disturbances. A 3D tracking task by a robot manipulator with visual feedback from uncalibrated stereo cameras is exemplified to show the formation of the instrumental system, estimation process and performance of the image Jacobian matrix and the design of the servo controller. Effectiveness of the proposed method and satisfactory tracking process can be found from extensive simulations and experiments provided in the paper.},
author = {Qian, Jiang Qian Jiang and Su, Jianbo Su Jianbo},
doi = {10.1109/ROBOT.2002.1013418},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Qian, Su - 2002 - Online estimation of image Jacobian matrix by Kalman-Bucy filter for uncalibrated stereo vision feedback.pdf:pdf},
isbn = {0-7803-7272-7},
issn = {10504729},
journal = {Proceedings 2002 IEEE International Conference on Robotics and Automation (Cat. No.02CH37292)},
number = {May},
pages = {562--567},
title = {{Online estimation of image Jacobian matrix by Kalman-Bucy filter for uncalibrated stereo vision feedback}},
volume = {1},
year = {2002}
}
@article{Song2010a,
author = {Song, Bo and Xi, Ning and Yang, Ruiguo and Lai, King Wai Chiu and Qu, Chengeng},
doi = {10.1109/NMDC.2010.5651906},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Song et al. - 2010 - On-line sensing and visual feedback for atomic force microscopy (AFM) based nano-manipulations.pdf:pdf},
isbn = {9781424488964},
journal = {2010 IEEE Nanotechnology Materials and Devices Conference, NMDC2010},
keywords = {Micro and Nano Robotics,Nanolithography},
month = {oct},
pages = {71--74},
publisher = {Ieee},
title = {{On-line sensing and visual feedback for Atomic Force Microscopy (AFM) based nano-manipulations}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5651906},
year = {2010}
}
@article{Dame2009,
abstract = {This paper proposes a new way to achieve feature point tracking using the entropy of the image. Sum of Squared Differences (SSD) is widely considered in differential trackers such as the KLT. Here, we consider another metric called Mutual Information (MI), which is far less sensitive to changes in the lighting condition and to a wide class of non-linear image transformation. Since mutual-information is used as an energy function to be maximized to track each points, a new feature selection, which is optimal for this metric, is proposed. Results under various complex conditions are presented. Comparison with the classical KLT tracker are proposed.},
author = {Dame, Amaury and Marchand, Eric},
doi = {10.1109/ICIP.2009.5414301},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dame, Marchand - 2009 - Optimal detection and tracking of feature points using mutual information.pdf:pdf},
isbn = {978-1-4244-5653-6},
journal = {2009 16th IEEE International Conference on Image Processing (ICIP)},
month = {nov},
number = {1},
pages = {3601--3604},
publisher = {Ieee},
title = {{Optimal detection and tracking of feature points using mutual information}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5414301},
year = {2009}
}
@article{Ishiyama1999,
author = {Ishiyama, Rui and Deguchi, Koichiro},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ishiyama, Deguchi - 1999 - Optimal Motion by Control for and Visual Rotation Servoing Decoupling Rui Ishiyama l Translation and Koichir.pdf:pdf},
pages = {116--123},
title = {{Optimal Motion by Control for and Visual Rotation Servoing Decoupling Rui Ishiyama * l Translation and Koichiro of a robot mounting a camera . It does not need 3D object models and is robust for image reading errors and noise . tained image , trajectory o}},
volume = {17},
year = {1999}
}
@article{Johnson2008,
abstract = {The driving precision landing requirement for the Autonomous Landing and Hazard Avoidance Technology project is to autonomously land within 100 m of a predetermined location on the lunar surface. Traditional lunar landing approaches based on inertial sensing do not have the navigational precision to meet this requirement. The purpose of Terrain Relative Navigation (TRN) is to augment inertial navigation by providing position or bearing measurements relative to known surface landmarks. From these measurements, the navigational precision can be reduced to a level that meets the 100 m requirement. There are three different TRN functions: global position estimation, local position estimation and velocity estimation. These functions can be achieved with active range sensing or passive imaging. This paper gives a survey of many TRN approaches and then presents some high fidelity simulation results for contour matching and area correlation approaches to TRN using active sensors. Since TRN requires an a-priori reference map, the paper concludes by describing past and future lunar imaging and digital elevation map data sets available for this purpose.},
author = {Johnson, Andrew E. and Montgomery, James F.},
doi = {10.1109/AERO.2008.4526302},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/NASA2.pdf:pdf},
isbn = {1424414881},
issn = {1095323X},
journal = {IEEE Aerospace Conference Proceedings},
title = {{Overview of Terrain Relative Navigation approaches for precise lunar landing}},
year = {2008}
}
@article{Akhtar2015,
abstract = {This paper presents an approach for designing path-following controllers for the kinematic model of car-like mobile robots using transverse feedback linearization with dynamic extension. This approach is applicable to a large class of paths and its effectiveness is experimentally demonstrated on a Chameleon R100 Ackermann steering robot. Transverse feedback linearization makes the desired path attractive and invariant, while the dynamic extension allows the closed-loop system to achieve the desired motion along the path.},
author = {Akhtar, A and Nielsen, C and Waslander, S L},
journal = {IEEE Transactions on Automatic Control},
keywords = {Control design,mobile robots,motion control,nonlinear control systems,nonlinear dynamical systems,nonlinear systems,robot control},
month = {apr},
number = {2},
pages = {269--279},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Path following using dynamic transverse feedabck linearization for ca-like robots}},
volume = {31},
year = {2015}
}
@article{Satici2013,
abstract = {This paper presents a novel navigation and control system for wheeled mobile robots that includes path planning, localization, and control. A path following control system is introduced that is capable of guiding and keeping the robot on a designated curve. Localization and velocity estimation are provided by a unique sensor fusion algorithm that incorporates vision, IMU and wheel encoder data. Stability analysis is provided for the control system, and experimental results are presented that prove the combined localization and control system performs with high accuracy. {\textcopyright} 2013 AACC American Automatic Control Council.},
author = {Satici, A and Tick, D and Shen, J and Gans, N},
doi = {10.1109/ACC.2013.6580824},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/Path-Following Control For Mobile Robots Localized Via Sensor-Fused Visual Homography.pdf:pdf},
isbn = {9781479901777},
issn = {07431619},
journal = {Proceedings of the American Control Conference},
keywords = {Control systems,Homographies,Mobile robots,Motion planning,Navigation and control,Path followi,Sensor data fusion},
pages = {6287--6293},
title = {{Path-following control for mobile robots localized via sensor-fused visual homography}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84883544842{\&}partnerID=40{\&}md5=7d213fe01a3829d80b911e99350a0457},
year = {2013}
}
@article{Chesi2009,
abstract = {Visual servoing consists of positioning a robot end-effector based on the matching of some object features in the image. However, due to the presence of image noise, this matching can never be ensured, hence introducing an error on the final location of the robot. This paper addresses the problem of estimating the worst-case location error introduced by image points matching. In particular, we propose some strategies for computing upper bounds and lower bounds of such an error according to several possible measures for certain image noise intensity and camera-object configuration. These bounds provide an admissible region of the sought worst-case location error, and hence allow one to establish performance limitation of visual servo systems. Some examples are reported to illustrate the proposed strategies and their results.},
author = {Chesi, Graziano and Yung, Ho Lam},
doi = {10.1109/ROBOT.2009.5152225},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chesi - 2009 - Performance limitation analysis in visual servo systems Bounding the location error introduced by image points matching.pdf:pdf},
isbn = {9781424427895},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
keywords = {Convex optimization,Image noise,Positioning accuracy,Visual servoing},
month = {may},
pages = {695--700},
publisher = {Ieee},
title = {{Performance limitation analysis in visual servo systems: Bounding the location error introduced by image points matching}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5152225},
year = {2009}
}
@article{Bakthavatchalam2013,
abstract = {In this paper, we propose a new type of visual features for visual servoing: photometric moments. These global features do not require any segmentation, matching or tracking steps. The analytical form of the interaction matrix is developed in closed form for these features. Results from experiments carried out with photometric moments have been presented. The results validate our modelling and the control scheme. They perform well for large camera displacements and are endowed with a large convergence domain. From the properties exhibited, photometric moments hold promise as better candidates for IBVS over currently existing geometric and pure luminance features.},
author = {Bakthavatchalam, Manikandan and Chaumette, Francois and Marchand, Eric},
doi = {10.1109/ICRA.2013.6631326},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bakthavatchalam, Chaumette, Marchand - 2013 - Photometric moments New promising candidates for visual servoing.pdf:pdf},
isbn = {9781467356411},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {5241--5246},
title = {{Photometric moments: New promising candidates for visual servoing}},
year = {2013}
}
@article{Collewet2011,
abstract = {This paper proposes a new way to achieve robotic tasks by two-dimensional (2-D) visual servoing. Indeed, instead of using classical geometric features such as points, straight lines, pose, or a homography, as is usually done, the luminance of all pixels in the image is considered here. The main advantage of this new approach is that it requires no tracking or matching process. The key point of our approach relies on the analytic computation of the interaction matrix. This computation is based either on a temporal luminance-constancy hypothesis or on a reflection model so that complex illumination changes can be considered. Experimental results on positioning and tracking tasks validate the proposed approach and show its robustness to approximated depths, low-textured objects, partial occlusions, and specular scenes. They also showed that luminance leads to lower positioning errors than a classical visual servoing based on 2-D geometric visual features.},
author = {Collewet, Christophe and Marchand, Eric},
doi = {10.1109/TRO.2011.2112593},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Collewet, Marchand - 2011 - Photometric visual servoing.pdf:pdf},
isbn = {0249-6399},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Cost function,optimization,photometry,visual features,visual servoing},
number = {4},
pages = {828--834},
title = {{Photometric visual servoing}},
volume = {27},
year = {2011}
}
@article{Collewet2011,
abstract = {This paper proposes a new way to achieve robotic tasks by two-dimensional (2-D) visual servoing. Indeed, instead of using classical geometric features such as points, straight lines, pose, or a homography, as is usually done, the luminance of all pixels in the image is considered here. The main advantage of this new approach is that it requires no tracking or matching process. The key point of our approach relies on the analytic computation of the interaction matrix. This computation is based either on a temporal luminance-constancy hypothesis or on a reflection model so that complex illumination changes can be considered. Experimental results on positioning and tracking tasks validate the proposed approach and show its robustness to approximated depths, low-textured objects, partial occlusions, and specular scenes. They also showed that luminance leads to lower positioning errors than a classical visual servoing based on 2-D geometric visual features.},
author = {Collewet, Christophe and Marchand, Eric},
doi = {10.1109/TRO.2011.2112593},
file = {:C$\backslash$:/Users/yoshi/Dropbox/papers/Photometric visual servoing.pdf:pdf},
isbn = {0249-6399},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Cost function,optimization,photometry,visual features,visual servoing},
number = {4},
pages = {828--834},
title = {{Photometric visual servoing}},
volume = {27},
year = {2011}
}
@article{Krupinski2012,
author = {Krupinski, S and Allibert, Guillaume},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/Pipeline tracking for fully-actuated autonomous underwater vehicle using visual servo control.pdf:pdf},
isbn = {9781457710964},
journal = {American Control {\ldots}},
title = {{Pipeline tracking for fully-actuated autonomous underwater vehicle using visual servo control}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6315438},
year = {2012}
}
@article{Tahri2005,
abstract = { Moments are generic (and usually intuitive) descriptors that can be computed from several kinds of objects, defined either from closed contours or from a set of points. In this paper, we present improvements in image-based visual servo using image moments. First, the analytical form of the interaction matrix related to the moments computed from a set of coplanar points is derived, and we show that it is different from the form obtained previously, using coplanar closed contours. Six visual features are selected to design a decoupled control scheme when the object is parallel to the image plane. This nice property is then generalized to the case where the desired object position is not parallel to the image plane. Finally, experimental results are presented to illustrate the validity of our approach and its robustness, with respect to modeling errors.},
author = {Tahri, Omar and Chaumette, Fran{\c{c}}ois},
doi = {10.1109/TRO.2005.853500},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/Point-Based and Region-Based Image Moments for Visual Servoing of Planer Objects.pdf:pdf},
isbn = {1552-3098},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Image moment,Invariant,Visual servoing},
number = {6},
pages = {1116--1127},
title = {{Point-based and region-based image moments for visual servoing of planar objects}},
volume = {21},
year = {2005}
}
@article{Tahri2005a,
abstract = { Moments are generic (and usually intuitive) descriptors that can be computed from several kinds of objects, defined either from closed contours or from a set of points. In this paper, we present improvements in image-based visual servo using image moments. First, the analytical form of the interaction matrix related to the moments computed from a set of coplanar points is derived, and we show that it is different from the form obtained previously, using coplanar closed contours. Six visual features are selected to design a decoupled control scheme when the object is parallel to the image plane. This nice property is then generalized to the case where the desired object position is not parallel to the image plane. Finally, experimental results are presented to illustrate the validity of our approach and its robustness, with respect to modeling errors.},
author = {Tahri, Omar and Chaumette, Fran{\c{c}}ois},
doi = {10.1109/TRO.2005.853500},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/Point-based and Region-based Image Moments for Visual Servoing of Planer Objects.pdf:pdf},
isbn = {1552-3098},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Image moment,Invariant,Visual servoing},
number = {6},
pages = {1116--1127},
title = {{Point-based and region-based image moments for visual servoing of planar objects}},
volume = {21},
year = {2005}
}
@article{Ozato2011,
author = {Ozato, Atsushi and Maru, Noriaki},
doi = {10.1299/kikaic.77.460},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ozato, Maru - 2011 - Position and Orientation Control of Omnidirectional Mobile Robot by Linear Visual Servoing.pdf:pdf},
issn = {1884-8354},
journal = {TRANSACTIONS OF THE JAPAN SOCIETY OF MECHANICAL ENGINEERS Series C},
keywords = {binocular visual space,linear visual servoing,omnidirectional mobile robot,position and orientation},
number = {774},
pages = {460--469},
title = {{Position and Orientation Control of Omnidirectional Mobile Robot by Linear Visual Servoing}},
url = {http://joi.jlc.jst.go.jp/JST.JSTAGE/kikaic/77.460?from=CrossRef},
volume = {77},
year = {2011}
}
@article{Shiraishib,
author = {Shiraishi, Takayuki and Fujimoto, Hiroshi},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shiraishi - Unknown - Positioning Control for Piezo-scanner using Iterative Learning Control based on PTC.pdf:pdf},
journal = {Iic},
number = {8},
pages = {1--6},
title = {{Positioning Control for Piezo-scanner using Iterative Learning Control based on PTC}},
year = {2010}
}
@article{Hashimoto2000,
author = {Hashimoto, Koichi and Tanaka, Kouhei and Noritsugu, Toshiro},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/Potential Problems in Visual Servo.pdf:pdf},
isbn = {0780364562},
number = {2},
pages = {2189--2194},
title = {{Potential Problems in Visual Servo}},
year = {2000}
}
@misc{,
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Powellのハイブリッド法における最小二乗法アルゴリズム.pdf.pdf:pdf},
title = {{Powellのハイブリッド法における最小二乗法アルゴリズム.pdf}}
}
@article{Mitsuda1998,
abstract = {In the drive towards miniaturization in manufacturing, accuracy in$\backslash$npositioning minute objects by camera is vital. For visual servoing, the$\backslash$nrapid and robust detections of features in camera images is also$\backslash$nessential to production line efficiency. Template matching provides$\backslash$nflexibility in achieving this, often lacked by other methods, because it$\backslash$navoids the need to set object-specific parameters. Unfortunately,$\backslash$nstandard methods of template matching require much calculation,$\backslash$nespecially for detecting feature rotation. The delay this causes means$\backslash$nthat for many applications template matching provides too slow a source$\backslash$nof visual feedback. As an alternative, we propose a new method of$\backslash$ndetecting the translation and rotation of a feature from coarse optical$\backslash$nflow, we and apply it to visual servoing. Coarse optical flow is derived$\backslash$nfrom the difference in intensity between a region of the initial and$\backslash$ncurrent image and their pixel-by-pixel intensity gradients. Unlike$\backslash$ntemplate matching, our method can detect large rotations with relatively$\backslash$nlittle calculation. Image resolution is then adjusted from coarse to$\backslash$nfine. Subpixel accuracy results in a 100 fold improvement in precision$\backslash$n(by area). We show experimental results for precise planar positioning$\backslash$n},
author = {Mitsuda, T. and Miyazaki, Y. and Maru, N. and MacDorman, K.F. and Miyazaki, F.},
doi = {10.1109/IROS.1998.727275},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mitsuda et al. - 1998 - Precise planar positioning using visual servoing based on coarse optical flow.pdf:pdf},
isbn = {0-7803-4465-0},
journal = {Proceedings. 1998 IEEE/RSJ International Conference on Intelligent Robots and Systems. Innovations in Theory, Practice and Applications (Cat. No.98CH36190)},
number = {October},
title = {{Precise planar positioning using visual servoing based on coarse$\backslash$noptical flow}},
volume = {2},
year = {1998}
}
@article{Li2014,
abstract = {? In this paper, a novel visual servo regulation strategy, which is based on the projection homography matrix, is presented for a wheeled mobile robot in the presence of unknown camera intrinsic parameters. Specifically, the deriva-tion of the projection homography matrix is described and then estimated without being restricted by the scale factor. Subsequently, to deal with the unknown depth information and unknown camera intrinsic parameters, an adaptive controller is carefully developed by utilizing the entities of the projection homography matrix to drive the mobile robot to the desired position. Using Lyapunov techniques, rigorous stability analysis is conducted. Afterwards, a controller for the angular velocity is further designed to regulate the orientation error of the mobile robot. The proposed method does not need any complex pose estimation algorithms, and it thus avoids the confusion made by the two solutions provided by homography decomposition methods. The performance of the proposed uncalibrated visual servoing strategy is further validated by simulation results.},
author = {Li, Baoquan and Fang, Yongchun and Zhang, Xuebo},
doi = {10.1109/CDC.2014.7039719},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/Projection Homography Based Uncalibrated Visual Servoing of Wheeled Mobile Robots.pdf:pdf},
isbn = {9781467360883},
issn = {07431546},
journal = {Proceedings of the IEEE Conference on Decision and Control},
number = {February},
pages = {2167--2172},
title = {{Projection homography based uncalibrated visual servoing of wheeled mobile robots}},
volume = {2015-February},
year = {2014}
}
@article{Miyajima,
author = {Miyajima, Takayuki},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Miyajima - Unknown - Proposal of Torque Feedforward Control with Voltage Phase Operation for SPMSM.pdf:pdf},
number = {4},
pages = {1--6},
title = {{Proposal of Torque Feedforward Control with Voltage Phase Operation for SPMSM}}
}
@article{Gao2014a,
abstract = {Recently, the needs of underwater robot used for many kinds of underwater work become higher and higher. In order to control the motion of underwater robot automatically, it is an indispensable to measure its position correctly in real time. Conventional systems are based on time difference or phase lag. However, they must use expensive components, such as transponders or atomic clock, and its system becomes complex. Additionally they require a lot of signal-processing time, so they cannot be used for motion control. As a result, there exist no system which can be used for automatic motion control of underwater robot. Since 2008, we have proposed a new positioning system based on sound propagation loss and sensor network. In this system, we set many buoys that install sound reception unit, GPS receiving equipment and sensor network system on the surface of water. We use sound propagation loss to calculate the distance between robot and buoys, and with combining distance and position data for each buoy, we estimate robot position. In former study, we found that we can get higher SN ratio of signal to measure distance with using the sound of multiple frequency, we proposed a new distance measurement method M{\_}SPL. It was introduced on the paper of “Underwater Acoustics Positioning System Based on Propagation Loss and Sensor Network” (OCEANS 2012 KOREA). In this paper, we introduce the design and construction of prototype system for this positioning system as the results of latest study. It contains transmission unit, reception unit and center unit. Transmission unit follows with principle of M{\_}SPL, is set into underwater robot used as sound source. We also complete reception unit to be used to receive sound signal and calculate distance automatically. Finally, center unit receive distance and each buoy position data what are sent from reception unit, position can be calculated by using these data. We confirmed the performance of transmission unit and r- ceiving unit in water-tank, and it showed that both units can be used effectively. Now we are on a final test of this system in open sea for the actual use of surveying sea floor with the underwater robot made by NAGASAKI UNIVERSITY in Japan.},
author = {Gao, Xiujing and Zhang, Feifei and Ito, Masanori and Mishima, Kiyoshi and Onodera, Ribun and Inagawa, Naohiro and Yamamoto, Ikuo},
doi = {10.1109/OCEANS-TAIPEI.2014.6964319},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gao, Zhang, Ito - 2014 - Prototype of Positioning System for Automatic Motion Control of Underwater Robot.pdf:pdf},
isbn = {9781479936465},
journal = {Oceans 2014 - Taipei},
keywords = {Automatic Motion Control,D-SPL,M-SPL,Mixed Wave,Multiply Frequency,Prototype System,Sensor Network,Sound Propagation Loss,Underwater Robot},
title = {{Prototype of positioning system for automatic motion control of underwater robot}},
year = {2014}
}
@article{Chen2009,
abstract = {We present a simple approach for vision-based path following for a mobile robot. Based upon a novel concept called the funnel lane, the coordinates of feature points during the replay phase are compared with those obtained during the teaching phase in order to determine the turning direction. Increased robustness is achieved by coupling the feature coordinates with odometry information. The system requires a single off-the-shelf, forward-looking camera with no calibration (either external or internal, including lens distortion). Implicit calibration of the system is needed only in the form of a single controller gain. The algorithm is qualitative in nature, requiring no map of the environment, no image Jacobian, no homography, no fundamental matrix, and no assumption about a flat ground plane. Experimental results demonstrate the capability of real-time autonomous navigation in both indoor and outdoor environments and on flat, slanted, and rough terrain with dynamic occluding objects for distances of hundreds of meters. We also demonstrate that the same approach works with wide-angle and omnidirectional cameras with only slight modification.},
author = {Chen, Zhichao and Birchfield, Stanley T.},
doi = {10.1109/TRO.2009.2017140},
file = {:C$\backslash$:/Users/yoshi/Dropbox/Qualitative Vision-Based Path Following.pdf:pdf},
isbn = {1552-3098},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Control,Feature tracking,Mobile robot navigation,Vision-based navigation},
number = {3},
pages = {749--754},
title = {{Qualitative vision-based path following}},
volume = {25},
year = {2009}
}
@article{Kagami2010,
author = {Kagami, Shingo},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/センサを用いない距離情報の可視化.pdf:pdf},
pages = {3--6},
title = {{Range-Finding Projectors : Range-Finding Projectors : Visualizing Range Information}},
volume = {5},
year = {2010}
}
@article{Kagami2010a,
abstract = {We describe sensorless visualizing techniques of 3D range information of object surfaces, in which the range information is projected directly onto the real-world surface by projectors. The proposed system consists of two projectors that merely project still images. Two techniques, namely, contour map projection based on the moire method and pseudo-color map projection based on complementary colors, are shown to be used simultaneously.},
author = {Kagami, Shingo},
doi = {10.1109/ISMAR.2010.5643586},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kagami - 2010 - Range-finding projectors Visualizing range information without sensors.pdf:pdf},
isbn = {9781424493449},
journal = {9th IEEE International Symposium on Mixed and Augmented Reality 2010: Science and Technology, ISMAR 2010 - Proceedings},
keywords = {H.5.1 [information systems]: information interfaces and presentation-multimedia information systems-Artificial, augmented, and virtual realities,I.4.9 [computing methodologies]: image processing and computer vision-applications},
pages = {239--240},
title = {{Range-finding projectors: Visualizing range information without sensors}},
year = {2010}
}
@article{Choi2008,
author = {Choi, Changhyun and Baek, Seung-min and Lee, Sukhan and Member, Fellow},
doi = {10.1109/IROS.2008.4650710},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/Real-time 3D Object Pose Estimation and Tracking for Natural Landmark based Visualservo.pdf:pdf},
isbn = {978-1-4244-2057-5},
journal = {2008 IEEE/RSJ International Conference on Intelligent Robots and Systems},
keywords = {Recognition,Visual Servoing,Visual Tracking},
pages = {3983--3989},
title = {{Real-time 3D object pose estimation and tracking for natural landmark based visual servo}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4650710},
year = {2008}
}
@article{Lothe2010,
abstract = {In this system paper, we propose a real-time car localisation process in dense urban areas by using a single perspective camera and a priori on the environment. To tackle this problem, it is necessary to solve two well-known monocular SLAM limitations: scale factor drift and error accumulation. The proposed idea is to combine a monocular SLAM process based on bundle adjustment with simple knowledge, i.e. the position and orientation of the camera with regard to the road and a coarse 3D model of the environment, as those provided by GIS database. First, we show that, thanks to specific SLAM-based constraints, the road homography can be expressed only with respect to the scale factor parameter. This allows the scale factor to be robustly and frequently estimated. Then, we propose to use the global information brought by 3D city models in order to correct the monocular SLAM error accumulation. Even with coarse 3D models, turnings give enough geometrical constraints to allow fitting the reconstructed 3D point cloud with the 3D model. Experiments on large-scale sequences (several kilometres) show that the entire process permits the real-time localisation of a car in city centre, even in real traffic condition.},
author = {Lothe, Pierre and Bourgeois, Steve and Royer, Eric and Dhome, Michel and Naudet-Collette, Sylvie},
doi = {10.1109/CVPR.2010.5540127},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/Real-Time Vehicle Global Localisation with a Single Camera in Dense Urban Areas Exploitation of Coarse 3D City Modals.pdf:pdf},
isbn = {9781424469840},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {863--870},
title = {{Real-time vehicle global localisation with a single camera in dense urban areas: Exploitation of coarse 3D city models}},
year = {2010}
}
@article{Tokuyama,
author = {Tokuyama, Kyota and Fujimoto, Hiroshi},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tokuyama, Fujimoto - Unknown - Reducing Impact Force Control by Impact Model for Stage with Decouplable Structure of Coarse and Fine Par.pdf:pdf},
number = {c},
pages = {2--7},
title = {{Reducing Impact Force Control by Impact Model for Stage with Decouplable Structure of Coarse and Fine Part}}
}
@article{Zong2008,
abstract = {According to the theory of hybrid dynamic system, a new method-switching approach, is applied to robot visual servo system. Two visual servo sub-controllers are introduced in detail, homography-based controller and affine approximation controller. The system switches between the two controllers according to a switching rule. The experiment result shows that this system works well, and can realize visual servo task exactly.},
author = {Zong, Xiao Ping and Huai, Xiao Li and Wang, Pei Guang and Hao, Lei},
doi = {10.1109/ICMLC.2008.4620753},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/ROBOT VISUAL SERVO CONTROL BASED ON SWITCHINGAPPROACH.pdf:pdf},
isbn = {9781424420964},
journal = {Proceedings of the 7th International Conference on Machine Learning and Cybernetics, ICMLC},
keywords = {Affine transformation,Homography matrix,Switching control,Visual servo},
number = {July},
pages = {2104--2108},
title = {{Robot visual servo control based on switching approach}},
volume = {4},
year = {2008}
}
@article{Sano2013,
abstract = {It is desirable for a container crane to operate smoothly and quickly. For this purpose, the control system of a container crane should be capable of antisway control for suppressing vibrations. A vision sensor system is often used to detect the sway angle. However, since a control system with a vision sensor has a delay time when determining the angle, it sometimes leads to deterioration of control performance owing to the delay time. In order to overcome this problem, this paper proposes a new antisway crane control system based on a dual-state observer with sensor-delay correction. However, because of nonlinear friction in the crane, the estimation accuracy achieved by using the observer is poor. To overcome this problem, this paper proposes a disturbance observer considering friction disturbance. The control performance and effectiveness of the proposed robust control system based on the estimated information are shown to be satisfactory by experimental results. (c) 2013 Wiley Periodicals, Inc. Electr Eng Jpn, 184(3): 3646, 2013; Published online in Wiley Online Library (wileyonlinelibrary.com). DOI 10.1002/eej.22412},
author = {Sano, Hiroki and Sato, Kentaro and Ohishi, Kiyoshi and Miyazaki, Toshimasa},
doi = {10.1002/eej.22412},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/AntiSwayControl.pdf:pdf},
isbn = {0424-7760},
issn = {04247760},
journal = {Electrical Engineering in Japan (English translation of Denki Gakkai Ronbunshi)},
keywords = {crane control,disturbance observer,model error,robust stability,state observer,vibration suppression},
number = {3},
pages = {36--46},
title = {{Robust design of vibration suppression control system for crane using sway angle observer considering friction disturbance}},
volume = {184},
year = {2013}
}
@article{Espiau2002,
author = {Espiau, Fransois-xavier and Malis, Ezio and Rives, Patrick},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/Robust features tracking for robotic applications toward212D visual servoing with nature images.pdf:pdf},
isbn = {0780372727},
number = {May},
pages = {574--579},
title = {{Robust features tracking for robotic applications : towards 2iD visual servoing with natural images}},
year = {2002}
}
@article{Sig,
author = {Sig, Ipsj and Report, Technical},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sig, Report - Unknown - Robust Image Matching Using Edge Based Eigen Template Method.pdf:pdf},
keywords = {image feature extraction,image recognition image estimation},
pages = {1--7},
title = {{Robust Image Matching Using Edge Based Eigen Template Method}}
}
@article{Shademan2010,
abstract = {This paper addresses robust estimation of the uncalibrated visual-motor Jacobian for an image-based visual servoing (IBVS) system. The proposed method does not require knowledge of model or system parameters and is robust to outliers caused by various visual tracking errors, such as occlusion or mis-tracking. Previous uncalibrated methods are not robust to outliers and assume that the visual-motor data belong to the underlying model. In unstructured environments, this assumption may not hold. Outliers to the visual-motor model may deteriorate the Jacobian, which can make the system unstable or drive the arm in the wrong direction. We propose to apply a statistically robust M-estimator to reject the outliers. We compare the quality of the robust Jacobian estimation with the least squares-based estimation. The effect of outliers on the estimation quality is studied through MATLAB simulations and eye-in-hand visual servoing experiments using a WAM arm. Experimental results show that the Jacobian estimated by robust M-estimation is robust when up to 40{\%} of the visual-motor data are outliers.},
author = {Shademan, Azad and Farahmand, Amir Massoud and J{\"{a}}gersand, Martin},
doi = {10.1109/ROBOT.2010.5509911},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/Robust Jacobian Estimation for Uncalibrated Visual Servoing.pdf:pdf},
isbn = {9781424450381},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {5564--5569},
title = {{Robust Jacobian estimation for uncalibrated visual servoing}},
year = {2010}
}
@article{Chien,
author = {Chien, Loy Hui and Aoki, Takafumi},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chien, Aoki - Unknown - Robust Motion Estimation for Video Sequences Based on Phase-Only Correlation.pdf:pdf},
keywords = {an im-,full search,hierarchical search,in block matching algorithms,its simplicity,phase-only correlation,robust motion estimation,sad},
pages = {441--446},
title = {{Robust Motion Estimation for Video Sequences Based on Phase-Only Correlation}},
year = {2004}
}
@article{,
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - {\v{S}}{\`{E}} ‰{\ae} ?{\`{I}}{\v{Z}}.pdf:pdf},
title = {{{\v{S}}{\`{E}} ‰{\ae} ?{\`{I}}{\v{Z}}}}
}
@article{Dame2012,
abstract = {In this paper, we present a direct image registration approach that uses mutual information (MI) as a metric for alignment. The proposed approach is robust and gives an accurate estimation of a set of 2-D motion parameters in real time. MI is a measure of the quantity of information shared by signals. Although it has the ability to perform robust alignment with illumination changes, multimodality, and partial occlusions, few works have proposed MI-based applications related to spatiotemporal image registration or object tracking in image sequences because of some optimization problems, which we will explain. In this paper, we propose a new optimization method that is adapted to the MI cost function and gives a practical solution for real-time tracking. We show that by refining the computation of the Hessian matrix and using a specific optimization approach, the registration results are far more robust and accurate than the existing solutions, with the computation also being cheaper. A new approach is also proposed to speed up the computation of the derivatives and keep the same optimization efficiency. To validate the advantages of the proposed approach, several experiments are performed.},
author = {Dame, Amaury and Marchand, Eric},
doi = {10.1109/TIP.2012.2199124},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dame, Marchand - 2012 - Second-order optimization of mutual information for real-time image registration.pdf:pdf},
isbn = {1941-0042 (Electronic)$\backslash$r1057-7149 (Linking)},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Mutual information (MI),optimization,registration,tracking},
month = {sep},
number = {9},
pages = {4190--4203},
pmid = {22588592},
title = {{Second-order optimization of mutual information for real-time image registration}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22588592},
volume = {21},
year = {2012}
}
@article{Sakata,
author = {Sakata, Koichi and Saiki, Kazuaki and Corporation, Nikon and Fujimoto, Hiroshi},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sakata et al. - Unknown - Self Resonance Cancellation using Multiple Sensors for Ballscrew Driven Stage Ms C Js B.pdf:pdf},
number = {1},
pages = {521--526},
title = {{Self Resonance Cancellation using Multiple Sensors for Ballscrew Driven Stage Ms + C Js + B}}
}
@article{Li2015,
author = {Li, Ping and Garratt, Matthew and Lambert, Andrew},
doi = {10.1109/ICCAS.2015.7364912},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/A homography-based visual inertial fusion method for robust sensing of a Micro Aerial Vehicle.pdf:pdf},
isbn = {978-8-9932-1508-3},
journal = {Control, Automation and Systems (ICCAS), 2015 15th International Conference on},
keywords = {optic flow,unmanned aerial vehicle,visual inertial fusion},
pages = {231--236},
title = {{Sensing and control of a quadrotor using a visual inertial fusion method}},
year = {2015}
}
@article{Liu2007,
author = {Liu, Lianqing and Xi, Ning and Luo, Yilun and Zhang, Jiangbo and Li, Guangyong},
doi = {10.1109/IROS.2007.4399353},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2007 - Sensor referenced guidance and control for robotic nanomanipulation.pdf:pdf},
isbn = {1424409128},
journal = {IEEE International Conference on Intelligent Robots and Systems},
month = {oct},
pages = {578--583},
publisher = {Ieee},
title = {{Sensor referenced guidance and control for robotic nanomanipulation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4399353},
year = {2007}
}
@article{Hamada2012,
abstract = {Toyota Motor Corporation is developing several types of environmentally friendly vehicles, such as hybrid (HV), electric (EV), plug-in hybrid (PHV), and fuel-cell hybrid (FCHV) vehicles, as part of its measures to address environmental issues. Hybrid technology is the key technology used in all these environmentally friendly vehicles, and this technology uses a wide variety of power electronics components. For its 3rd generation HVs, Toyota developed a direct cooling structure for the intelligent power module (IPM), as well as a trench gate structure and thin wafer technology for Si insulated gate bipolar transistors (IGBTs). However, these developments are approaching the limits of loss performance of Si power semiconductors, and it will soon be difficult to further improve the overall performance of components that utilize control and module technology. For these reasons, SiC is attracting attention as a potential new breakthrough material for the development of the next generation of power electronic components. Compared to Si, SiC devices have the merits of size and loss reduction, and allow the adoption of simpler cooling structures. In contrast, various issues must be resolved before SiC devices can be practically adopted. These include securing sufficient reliability, developing high-temperature materials, reducing costs, and the like. This paper describes the current status and future issues of power electronics research and development in the Toyota group, focusing on SiC devices and power module technology.},
author = {Hamada, K.},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hamada - 2012 - SiC Device and Power Module Technologies for Environmentally Friendly Vehicles.pdf:pdf},
isbn = {9783800734146},
keywords = {Cooling,Insulated gate bipolar transistors,Si insulated gate bipolar transistors,Si power semiconductors,SiC,SiC device,Silicon,Silicon carbide,Temperature measurement,Toyota Motor Corporation,Vehicles,automotive electronics,cooling,direct cooling structure,electric vehicle,environmental factors,environmental issues,environmentally friendly vehicles,fuel-cell hybrid vehicle,high-temperature materials,insulated gate bipolar transistors,intelligent power module,plug-in hybrid vehicle,power electronic components,power electronics components,power module technologies,power semiconductor devices,silicon compounds,thin wafer technology,trench gate structure},
pages = {1--6},
title = {{SiC Device and Power Module Technologies for Environmentally Friendly Vehicles}},
year = {2012}
}
@article{Dias2015,
author = {Dias, Andr{\'{e}} Seixas and Brites, Catarina and Ascenso, Jo{\~{a}}o and Pereira, Fernando},
doi = {10.1109/JSEN.2014.2355914},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dias et al. - 2015 - SIFT-Based Homographies for Efficient Multiview Distributed Visual Sensing.pdf:pdf},
issn = {1530437X},
journal = {IEEE Sensors Journal},
keywords = {Distributed video coding,SIFT,fusion,homography,multiview,visual sensors},
number = {5},
pages = {2643--2656},
title = {{SIFT-Based Homographies for Efficient Multiview Distributed Visual Sensing}},
volume = {15},
year = {2015}
}
@article{Welch2006,
abstract = {The deployment of solar-based electricity generation, especially in the form of photovoltaics (PVs), has increased markedly in recent years due to a wide range of factors including concerns over greenhouse gas emissions, supportive government policies, and lower equipment costs. Still, a number of challenges remain for reliable, efficient integration of solar energy. Chief among them will be developing new tools and practices that manage the variability and uncertainty of solar power.},
archivePrefix = {arXiv},
arxivId = {1511.05877},
author = {Tuohy, Aidan and Zack, John and Haupt, Sue Ellen and Sharp, Justin and Ahlstrom, Mark and Dise, Skip and Grimit, Eric and Mohrlen, Corinna and Lange, Matthias and Casado, Mayte Garcia and Black, Jon and Marquis, Melinda and Collier, Craig},
chapter = {8},
doi = {10.1109/MPE.2015.2461351},
editor = {{Acm Press}, Addison-WesleyEditor},
eprint = {1511.05877},
institution = {University of North Carolina at Chapel Hill},
isbn = {13504827 (ISSN)},
issn = {15407977},
journal = {IEEE Power and Energy Magazine},
keywords = {Photovoltaics,Predictive models,Real-time systems,Satellites,Solar energy,Wind forecasting,Wind power generation,Wind speed},
number = {6},
pages = {50--59},
pmid = {20578276},
publisher = {Citeseer},
title = {{Solar Forecasting: Methods, Challenges, and Performance}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.79.6578{\&}rep=rep1{\&}type=pdf},
volume = {13},
year = {2015}
}
@article{Corke2010,
abstract = {This paper presents a formulation of image-based visual servoing (IBVS) for a spherical camera where coordinates are parameterized in terms of colatitude and longitude: IBVS-Sph. The image Jacobian is derived and simulation results are presented for canonical rotational, translational as well as general motion. Problems with large rotations that affect the planar perspective form of IBVS are not present on the sphere, whereas the desirable robustness properties of IBVS are shown to be retained. We also describe a structure from motion (SfM) system based on camera-centric spherical coordinates and show how a recursive estimator can be used to recover structure. The spherical formulations for IBVS and SfM are particularly suitable for platforms, such as aerial and underwater robots, that move in SE(3).},
author = {Corke, Peter I.},
doi = {10.1109/ROBOT.2010.5509199},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/Spherical Image-Based Visual Servo and Structure Estimation.pdf:pdf},
isbn = {9781424450381},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {5550--5555},
title = {{Spherical image-based visual servo and structure estimation}},
year = {2010}
}
@article{,
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/Templage Matchingの魅力.pdf:pdf},
title = {{SSII2013TS-Hashimoto.pdf}},
year = {2013}
}
@article{LamyAuRousseau2005,
abstract = {An efficient star pattern recognition algorithm is presented. The purpose of this algorithm is to make sure of the compatibility of the software and the imaging sensor noise level. The new CMOS APS sensors have not currently reached the same accuracy as the former CCD sensors in position as well as in magnitude determination, especially in the dynamic stages. This algorithm allows the system to recognize the star pattern 20{\%} faster than with reference algorithms. No false recognition has been noticed. Used databases have a size 5 to 10 times smaller, depending on other reference algorithms. Oriented triangles are used to compare the measured star pattern with the catalogue stars. The triangle's characterization criteria propose several solutions in a first time. A unique solution is selected by means of identification and validation methods in a second time. First results, presented hereinafter, are very encouraging, and this algorithm may be used in the future APS star trackers. APS star tracker robustness is significantly enhanced by this method during the critical navigation phases},
author = {{Lamy Au Rousseau}, G. and Bostel, J. and Mazari, B.},
doi = {10.1109/MAES.2005.1397146},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lamy Au Rousseau, Bostel, Mazari - 2005 - Star recognition algorithm for APS star tracker Oriented triangles.pdf:pdf},
issn = {08858985},
journal = {IEEE Aerospace and Electronic Systems Magazine},
number = {2},
pages = {27--31},
title = {{Star recognition algorithm for APS star tracker: Oriented triangles}},
volume = {20},
year = {2005}
}
@article{Levine2015,
abstract = {Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Sofiyanti, Nery and Fitmawati, Dyah Iriani and Roza, Andesba A.},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/End-to-End Training of Deep Visuomotor Policies.pdf:pdf},
isbn = {9780874216561},
issn = {10282092},
journal = {Bangladesh Journal of Plant Taxonomy},
keywords = {Blechnaceae,Indonesia,New species,Stenochlaena},
number = {2},
pages = {137--141},
pmid = {15003161},
title = {{Stenochlaena Riauensis (Blechnaceae), A new fern species from riau, Indonesia}},
volume = {22},
year = {2015}
}
@article{Inukai,
author = {Inukai, Kenji},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Inukai - Unknown - tc73 Tp6!￠{\pounds}?eYdV\w|{\S} 2 R¨U{\textcopyright}d?dV W y !{\#}`{\$}{\%}{\&}'P( u q iSfQ ?.pdf:pdf},
pages = {1--6},
title = {{tc73 Tp6!￠{\pounds}?eYdV\w|{\S} 2 R¨U{\textcopyright}d?dV W y !"{\#}`{\$}{\%}{\&}'P( u q iSfQ ?}}
}
@article{Chaumette1998,
abstract = {Visual servoing, using image-based control or position-based control, generally gives satisfactory results. However, in some cases, convergence and stability problems may occur. The aim of this paper is to emphasize these problems by considering an eye-in-hand system and a positioning task with respect to a static target which constrains the six camera degrees of freedom.},
author = {Chaumette, Francois Fran{\c{c}}ois},
doi = {10.1007/BFb0109658},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/potential problems of stability and convergence in image-based and position-based visual servoing.pdf:pdf},
isbn = {978-1-85233-025-5},
issn = {0170-8643},
journal = {The confluence of vision and control},
pages = {66--78},
title = {{The confluence of vision and control}},
url = {http://www.mendeley.com/catalog/potential-problems-stability-convergence-imagebased-positionbased-visual-servoing/},
volume = {237},
year = {1998}
}
@article{Sutoh2015,
abstract = {This article presents a comprehensive path-planning method for lunar and planetary exploration rovers. In this method, two new elements are introduced as evaluation indices for path planning: 1) determined by the rover design and 2) derived from a target environment. These are defined as the rover's internal and external elements, respectively. In this article, the rover's locomotion mechanism and insolation (i.e., shadow) conditions were considered to be the two elements that ensure the rover's safety and energy, and the influences of these elements on path planning were described. To examine the influence of the locomotion mechanism on path planning, experiments were performed using track and wheel mechanisms, and the motion behaviors were modeled. The planned paths of the tracked and wheeled rovers were then simulated based on their motion behaviors. The influence of the insolation condition was considered through path plan simulations conducted using various lunar latitudes and times. The simulation results showed that the internal element can be used as an evaluation index to plan a safe path that corresponds to the traveling performance of the rover's locomotion mechanism. The path derived for the tracked rover was found to be straighter than that derived for the wheeled rover. The simulation results also showed that path planning using the external element as an additional index enhances the power generated by solar panels under various insolation conditions. This path-planning method was found to have a large impact on the amount of power generated in the morning/evening and at high-latitude regions relative to in the daytime and at low-latitude regions on the moon. These simulation results suggest the effectiveness of the proposed path-planning method.},
author = {Sutoh, Masataku and Otsuki, Masatsugu and Wakabayashi, Sachiko and Hoshino, Takeshi and Hashimoto, Tatsuaki},
doi = {10.1109/MRA.2014.2381359},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/Comprehensive Path Planning for Lunar Exploration Rovers.pdf:pdf},
issn = {10709932},
journal = {IEEE Robotics and Automation Magazine},
keywords = {Lunar exploration,Mobile robots,Path planning,Planets,Space research,Space vehicles,Tracking},
number = {1},
pages = {22--33},
title = {{The right path: Comprehensive path planning for lunar exploration rovers}},
volume = {22},
year = {2015}
}
@article{Control2007,
author = {Control, Torque and Mounted, Redundant Manipulator and Using, Flexible Base and Matrix, Pseudoinverse},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/擬似逆行列を用いた柔軟ベース上冗長マニピュレータのトルク制御.pdf:pdf},
keywords = {flexible base manipulator,pseudoinverse matrix,redundant manipulator,singularity-consistent method,vibration suppression},
number = {07},
pages = {600--605},
title = {{Torque Control of a Redundant Manipulator Mounted on a Flexible Base Using the Pseudoinverse Matrix}},
year = {2007}
}
@article{Shen2015,
author = {Shen, Peiyao},
doi = {10.1109/CYBER.2015.7287926},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/Trajectory planning of omnidirectional mobile robots with active casters$\backslash$; Multi-motor coordination and singularity avoidance.pdf:pdf},
isbn = {9781479987306},
title = {{Trajectory planning of omnidirectional mobile robots with active casters : Multi-motor coordination and singularity avoidance}},
year = {2015}
}
@article{Y,
author = {{\'{Y}}, {\"{E}}},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/サンプル値制御理論２.pdf:pdf},
title = {{{\^{U}} ? ´ !}}
}
@article{Oo,
author = {{\^{O}}{\o}, {\"{O}}},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/CH01mainChapJ.pdf:pdf},
title = {{{\`{U}} {\`{u}}×{\o} , ???}}
}
@article{Vehicle1995,
author = {Vehicle, Unmanned Aerial and E-, Gyrosaucer I I},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vehicle, E- - 1995 - UAV (Unmanned Aerial Vehicle).pdf:pdf},
pages = {21--72},
title = {{UAV (Unmanned Aerial Vehicle)}},
year = {1995}
}
@article{Chen2006,
abstract = {A visual servo tracking controller is developed in this paper for a monocular camera system mounted on a unmanned aerial vehicle (UAV) to track a leading UAV with a fixed relative position and orientation. Specifically, by comparing the feature points on the leading UAV from a prerecorded desired image with the corresponding feature points in the live image, projective geometric relationships are exploited to construct a Euclidean homography. A theoretical framework is developed for homography based visual servo technique for the general case in which both the camera and the object are moving relative to an inertial coordinate frame. This technique can also be applied to visual estimation of velocity or Euclidean struction of a moving object by a moving camera. The information obtained by decomposing the Euclidean homography is used to develop a robust kinematic controller which eliminates the knowledge of the leading UAV's velocities. A Lyapunov-based analysis is used to show that the proposed control strategy achieves global uniformly ultimately bounded (GUUB) position and orientation tracking},
author = {Chen, Jian and Dawson, Darren M.},
doi = {10.1109/CDC.2006.377532},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Dawson - 2006 - UAV Tracking with a Monocular Camera.pdf:pdf},
isbn = {1-4244-0171-2},
issn = {01912216},
journal = {Proceedings of the 45th IEEE Conference on Decision and Control},
keywords = {Cameras,Communication system control,Control systems,Euclidean homography,Kinematics,Lyapunov methods,Lyapunov-based analysis,Mobile robots,Noise measurement,Servomechanisms,Servosystems,UAV tracking,Unmanned aerial vehicles,Visual servoing,global uniformly ultimately bounded position,inertial coordinate frame,monocular camera,motion estimation,orientation tracking,position control,projective geometric relationships,remotely operated vehicles,robust kinematic controller,tracking,unmanned aerial vehicle,velocity control,visual estimation,visual servo technique,visual servo tracking controller,visual servoing},
pages = {3873--3878},
publisher = {Ieee},
title = {{UAV Tracking with a Monocular Camera}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4177483},
year = {2006}
}
@inproceedings{Avanzini2010a,
abstract = {Environment, sustainable development as well as new transportation service emergence in urban areas are major concerns. Consequently, studies are currently intended to automate electric vehicles designed for applications in free access. An additional functionality that appears very attractive is vehicle platooning. In order to avoid oscillations within the fleet when completing this task, a global control strategy, supported by inter-vehicle communications, is investigated. Vehicle absolute localization is then needed and is here derived from monocular vision. These data are however expressed in a virtual vision world, slightly distorted with respect to the actual metric one. It has previously been shown that such a distortion can accurately be corrected on-line in different ways, considering telemetric or odometric data. These strategies have here been refined in order to provide optimal corrections. A comparative study, supported by simulations and full-scale experiments, is reported to exhibit benefits and performances of proposed approaches. {\textcopyright} 2010 IEEE.},
author = {Avanzini, P. and Thuilot, B. and Martinet, P.},
booktitle = {11th International Conference on Control, Automation, Robotics and Vision, ICARCV 2010},
doi = {10.1109/ICARCV.2010.5707943},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/Urban Vehicle Platoon using Monocular Vision Scale Factor Estimation.pdf:pdf},
isbn = {9781424478132},
keywords = {Automatic guided vehicles,Monocular vision,Nonlinear control,Platooning,Urban vehicles},
number = {December},
pages = {1803--1808},
title = {{Urban vehicle platoon using monocular vision: Scale factor estimation}},
year = {2010}
}
@article{Hosoda,
author = {Hosoda, Koh and Ishida, Kyohei and Asada, Minoru},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/Utilizing Affine Description for Adaptive Visual Servoing.pdf:pdf},
title = {{Utilizing Affine Description for Adaptive Visual Servoing Adaptive visual servoing Estimator of the relation between}}
}
@article{Jacobian,
author = {Spong, Mark W. and Hutchinson, Seth and Vidyasagar, Mathukumalli},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jacobian, Jacobian - Unknown - VELOCITY KINEMATICS ? THE MANIPULATOR.pdf:pdf},
journal = {Robot modeling and control},
pages = {125--162},
title = {{Velocity kinematics - the manipulator Jacobian}},
year = {2006}
}
@article{Asano,
author = {Asano, Yosuke and National, Kisarazu},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Asano, National - Unknown - Verification for stability of Visual Walk aiming at the target object by feature amplitude control.pdf:pdf},
pages = {1--6},
title = {{Verification for stability of Visual Walk aiming at the target object by feature amplitude control}}
}
@inproceedings{Ming2014,
author = {Cai, Ming and Sun, Xiu Xia and Xu, Song and Liu, Xi},
booktitle = {2014 IEEE Chinese Guidance, Navigation and Control Conference, CGNCC 2014},
doi = {10.1109/CGNCC.2014.7007276},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ming et al. - 2014 - Vision Aided INS for UAV Auto Landing Navigation Using SR-UKF Based on Two-View Homography.pdf:pdf},
isbn = {9781479946990},
pages = {518--522},
title = {{Vision aided INS for UAV auto landing navigation using SR-UKF based on two-view homography}},
year = {2015}
}
@article{Yol2014,
abstract = {This paper presents a method for localizing an Unmanned Aerial Vehicle (UAV) using georeferenced aerial images. Easily maneuverable and more and more affordable, UAVs have become a real center of interest. In the last few years, their utilization has significantly increased. Today, they are used for multiple tasks such as navigation, transportation or vigilance. Nevertheless, the success of these tasks could not be possible without a highly accurate localization which can, unfortunately be often laborious. Here we provide a multiple usage localization algorithm based on vision only. However, a major drawback with vision-based algorithms is the lack of robustness. Most of the approaches are sensitive to scene variations (like season or environment changes) due to the fact that they use the Sum of Squared Differences (SSD). To prevent that, we choose to use the Mutual Information (MI) which is very robust toward local and global scene variations. However, dense approaches are often related to drift disadvantages. Here, we solve this problem by using georeferenced images. The localization algorithm has been implemented and experimental results are presented demonstrating the localization of a hexarotor UAV fitted with a downward looking camera during real flight tests.},
author = {Yol, Aur{\'{e}}lien and Delabarre, Bertrand and Dame, Amaury and Dartois, Jean {\'{E}}mile and Marchand, Eric},
doi = {10.1109/IROS.2014.6943040},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yol, Delabarre, Dame - 2014 - Vision-based Absolute Localization for Unmanned Aerial Vehicles.pdf:pdf},
isbn = {9781479969340},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
number = {Iros},
pages = {3429--3434},
title = {{Vision-based absolute localization for unmanned aerial vehicles}},
url = {http://hal.inria.fr/hal-01010140/PDF/IROS14{\_}vfinal{\_}ayol.pdf},
volume = {14},
year = {2014}
}
@article{Benhimane2005,
abstract = {  In this paper, we present a complete system for car platooning using visual tracking. The visual tracking is achieved by directly estimating the projective transformation (in our case a homography) between a selected reference template attached to the leading vehicle and the corresponding area in the current image. The relative position and orientation of the servoed car with regard to the leading one is computed by decomposing the homography. The control objective is stated in terms of path following task in order to cope with the non-holonomic constraints of the vehicles. },
author = {Benhimane, Selim and Malis, Ezio and Rives, Patrick and Azinheira, Jos{\'{e}} Raul},
doi = {10.1109/ROBOT.2005.1570433},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/Vision-based Control for Car Platooning using Homography Decomposition.pdf:pdf},
isbn = {078038914X},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
keywords = {Car platooning,Non-holonomic vehicles,Real-time visual tracking,Vision-based control},
number = {April},
pages = {2161--2166},
title = {{Vision-based control for car platooning using homography decomposition}},
volume = {2005},
year = {2005}
}
@article{Water2015,
author = {Wang, Kai and Liu, Yunhui and Li, Luyang},
doi = {10.1109/TCST.2015.2403471},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/Vision-Based Tracking Control of Underactuated Water.pdf:pdf},
issn = {10636536},
journal = {IEEE Transactions on Control Systems Technology},
keywords = {Adaptive control,Visual servoing,trajectory tracking,underactuated water surface robots},
number = {6},
pages = {2391--2399},
title = {{Vision-Based Tracking Control of Underactuated Water Surface Robots Without Direct Position Measurement}},
volume = {23},
year = {2015}
}
@article{Chiu2011,
abstract = {In this paper, a vision-based flight control system that uses a skyline-detection algorithm is developed for application to small unmanned aerial vehicles. The skyline-detection algorithm can detect straight or uneven skylines. The system integrates a remote controller, a remotely controlled airplane, a camera, a wireless transmitter/receiver, a ground control computer, and the proposed skyline-detection algorithm to achieve automatic control of flight stability. Static and dynamic tests are conducted to validate the system. In the static tests, the average accuracy rate for skyline detection is 98.62{\%} based on five test videos. In the dynamic tests, straight and circular flights are used to verify lateral and longitudinal stability for the proposed flight control system. The experimental results demonstrate the performance and robustness of the algorithm and the feasibility and potential of a low-cost vision-only flight control system.},
author = {Chiu, Chung Cheng and Lo, Ching Tung},
doi = {10.1109/TVT.2011.2157545},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/Vision-Only Automatic Flight control for small UAVs.pdf:pdf},
issn = {00189545},
journal = {IEEE Transactions on Vehicular Technology},
keywords = {Automated vehicles,skyline detection,vision control},
number = {6},
pages = {2425--2437},
title = {{Vision-only automatic flight control for small UAVs}},
volume = {60},
year = {2011}
}
@article{Liu2013,
abstract = {Visual homing enables a mobile robot to move to a reference position using only visual information. The approaches that we present in this paper utilize matched image key points (e.g., scale-invariant feature transform) that are extracted from an omnidirectional camera as inputs. First, we propose three visual homing methods that are based on feature scale, bearing, and the combination of both, under an image-based visual servoing framework. Second, considering computational cost, we propose a simplified homing method which takes an advantage of the scale information of key-point features to compute control commands. The observability and controllability of the algorithm are proved. An outlier rejection algorithm is also introduced and evaluated. The results of all these methods are compared both in simulations and experiments. We report the performance of all related methods on a series of commonly cited indoor datasets, showing the advantages of the proposed method. Furthermore, they are tested on a compact dataset of omnidirectional panoramic images, which is captured under dynamic conditions with ground truth for future research and comparison.},
author = {Liu, Ming and Pradalier, C{\'{e}}dric and Siegwart, Roland},
doi = {10.1109/TRO.2013.2272251},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/Visual Homing From Scale With an Uncalibrated Omnidirectional Camera.pdf:pdf},
isbn = {1552-3098},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Omnidirectional camera,Topological visual navigation,Visual homing,Visual servoing},
number = {6},
pages = {1353--1365},
title = {{Visual homing from scale with an uncalibrated omnidirectional camera}},
volume = {29},
year = {2013}
}
@article{Kawai2006,
author = {Kawai, Hiroyuki and Murao, Toshiyuki},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kawai, Murao - 2006 - Visual Motion Observer ? based Pose Control with Input Saturation ? Application to Small Unmanned Aerial Vehicles.pdf:pdf},
keywords = {input saturation,lyapunov stability,manned aerial vehicle,un-,visual feedback control,visual motion observer},
number = {3},
pages = {639--645},
title = {{Visual Motion Observer ? based Pose Control with Input Saturation ? Application to Small Unmanned Aerial Vehicles ?}},
volume = {42},
year = {2006}
}
@article{K2009,
author = {Hashimoto, Koichi},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Institute of Systems, Control and Information Engineers NII-Electronic Library Service.pdf:pdf},
journal = {Institute of Systems, Control and Information Engineers NII-Electronic Library Service},
number = {11},
pages = {476--483},
title = {{Visual Servo - II - Fundamental Computer Vision (japanese)}},
volume = {53},
year = {2009}
}
@article{Hashi2010,
author = {Hashimoto, Koichi},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Institute of Systems, Control and Information Engineers NII-Electronic Library Service(5).pdf:pdf},
journal = {Institute of Systems, Control and Information Engineers NII-Electronic Library Service},
number = {5},
pages = {206--213},
title = {{Visual Servo - IV - Feature Based Visualservo (japanese)}},
volume = {54},
year = {2010}
}
@article{Hashimoto,
author = {Hashimoto, Koichi},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Institute of Systems, Control and Information Engineers NII-Electronic Library Service(6).pdf:pdf},
journal = {Institute of Systems, Control and Information Engineers NII-Electronic Library Service},
number = {7},
pages = {264--273},
title = {{Visual Servo - VI - Visual Tracking (japanese)}},
volume = {54},
year = {2010}
}
@article{Kim2009,
abstract = {This paper presents a three-axis vision motion sensor and its applications to visual servo control. The vision sensor is integrated with a three-axis piezo stage to form a visual servo control system that achieves nanometer resolution in all three},
author = {Kim, J.H. Jung H and Menq, Chia-hsiang and Member, Senior},
doi = {10.1109/TRO.2008.2003267},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/VisualServoControlAchevingNanometerResolutionInX-Y-Z.pdf:pdf},
issn = {1552-3098},
journal = {Robotics, IEEE Transactions on},
number = {1},
pages = {109--116},
title = {{Visual Servo Control Achieving Nanometer Resolution in X - Y - Z}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4757222},
volume = {25},
year = {2009}
}
@article{Karras2008,
abstract = {This paper describes a position-based visual servo control scheme designed for an underwater vehicle. The methodology proposes a path planning technique, which guar- antees that a flat target is kept in the camera optical field, while the vehicle avoids collision with the surface the target lays on. The vehicle pose (position and orientation) with respect to the target is obtained using a Laser Vision System (LVS). The LVS projects two laser dots in the image plane while it tracks the target using computer vision algorithms. The position of each laser dot in the image plane is directly related to the distance between the vehicle and the surface the target is located. The path planning strategy is based on the Artificial Potential Field method (APF). The attractive part of the APF is responsible for minimizing the error between the current vehicle position and the desired. The repulsive part of the APF restricts the target inside the camera optical field while keeps away the laser dots from image regions related to small distances between the vehicle and the surface the target is located. The steering control of the vehicle is achieved by feeding the computed points of the path planning into a Cartesian kinematic controller, which was slightly modified for the needs of the methodology. The overall efficiency of the system, was proved through an extensive experimental procedure, using a small Remotely Operated Vehicle (ROV) in a test tank.},
author = {Karras, G.C. and Kyriakopoulos, K.J.},
doi = {10.1109/IROS.2008.4650935},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/Visual Servo Control of an Underwater Vehicle Using a Laser Vision System.pdf:pdf},
isbn = {978-1-4244-2057-5},
journal = {2008 IEEE/RSJ International Conference on Intelligent Robots and Systems},
keywords = {Marine Robotics,Station Keeping,Visual Servoing},
pages = {4116--4122},
title = {{Visual servo control of an underwater vehicle using a Laser Vision System}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4650935},
year = {2008}
}
@article{Chaumette2006,
abstract = {This paper is the first of a two-part series on the topic of visual servo control using computer vision data in the servo loop to control the motion of a robot. In this paper, we describe the basic techniques that are by now well established in the field. We first give a general overview of the formulation of the visual servo control problem. We then describe the two archetypal visual servo control schemes: image-based and position-based visual servo control. Finally, we discuss performance and stability issues that pertain to these two schemes, motivating the second article in the series, in which we consider advanced techniques},
author = {Chaumette, Fra?ois and Hutchinson, Setrh},
doi = {10.1109/MRA.2006.250573},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chaumette, Hutchinson - 2006 - Visual servo control. I. Basic approaches.pdf:pdf},
isbn = {1070-9932},
issn = {10709932},
journal = {IEEE Robotics and Automation Magazine},
number = {4},
pages = {82--90},
title = {{Visual servo control. I. Basic approaches}},
volume = {13},
year = {2006}
}
@article{Chaumette2007,
abstract = {This article is the second of a two-part tutorial on visual servo control. In this tutorial, we have only considered velocity controllers. It is convenient for most of classical robot arms. However, the dynamics of the robot must of course be taken into account for high speed task, or when we deal with mobile nonholonomic or underactuated robots. As for the sensor, geometrical features coming from a classical perspective camera is considered. Features related to the image motion or coming from other vision sensors necessitate to revisit the modeling issues to select adequate visual features. Finally, fusing visual features with data coming from other sensors at the level of the control scheme will allow to address new research topics},
author = {Chaumette, Fran{\c{c}}ois and Hutchinson, Seth},
doi = {10.1109/MRA.2007.339609},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Magazine - 2007 - Visual Servo Control.pdf:pdf},
isbn = {1070-9932},
issn = {10709932},
journal = {IEEE Robotics and Automation Magazine},
keywords = {Robot control,Visual servo},
number = {1},
pages = {109--118},
title = {{Visual servo control. II. Advanced approaches [Tutorial]}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4141039},
volume = {14},
year = {2007}
}
@article{Omi2014,
author = {Omi, Kotaro and Kagami, Shingo and Hashimoto, Koichi},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Omi, Kagami, Hashimoto - 2014 - Visual servo for alignment of flexible sheets.pdf:pdf},
pages = {1--9},
title = {{Visual servo for alignment of flexible sheets}},
volume = {8},
year = {2014}
}
@article{Kinbara2006,
author = {Kinbara, Itsushi and Komada, Satoshi and Hirai, Junji},
doi = {10.1109/AMC.2006.1631734},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/Visual Servo of Active Camera and Manipulator with Simple On-line Calibration for Estimated Image Feature.pdf:pdf},
isbn = {0780395115},
journal = {International Workshop on Advanced Motion Control, AMC},
pages = {636--640},
title = {{Visual servo of active camera and manipulator with simple on-line calibration for estimated image feature}},
volume = {2006},
year = {2006}
}
@article{Kinbara2006a,
author = {Kinbara, Itsushi and Komada, Satoshi and Hirai, Junji},
doi = {10.1109/SICE.2006.315318},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/Visual Servo of Active Cameras and Manipulators by Time Delay Compensation of Image Features with Simple Online Calibration.pdf:pdf},
isbn = {8995003855},
journal = {2006 SICE-ICASE International Joint Conference},
keywords = {Active camera,Calibration,Estimated image feature,Visual servo},
pages = {5317--5322},
title = {{Visual servo of active cameras and manipulators by time delay compensation of image features with simple on-line calibration}},
year = {2006}
}
@article{Li2016,
abstract = {In this paper, a visual servo regulation strategy is designed for an uncalibrated camera system mounted on a wheeled mobile robot subject to nonholonomic mo- tion constraint, which can drive the mobile robot to the target pose with exponential convergence. Unlike existing methods, the proposed approach can work well without the camera intrinsic parameters being known in advance. Specifically, a novel fundamental matrix-based algorithm is first proposed to rotate the robot to point toward the de- sired position, with the camera intrinsic parameters esti- mated simultaneously by employing the fundamental matrix and a projection homography matrix. Subsequently, by uti- lizing the obtained camera intrinsic parameters, a straight- line motion controller is developed to drive the robot to the desired position, with the orientation of the robot always facing the target position. Another pure rotation controller is finally adopted to correct the orientation error. The ex- ponentially convergent properties of the visual servo errors are proven with mathematical analysis. The performance of the proposed uncalibrated visual servo regulation method is further validated by both simulation and experimental results.},
author = {Li, Baoquan and Fang, Yongchun and Zhang, Xuebo},
doi = {10.1109/TMECH.2015.2504098},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/Visual Servo Regulation of Wheeled Mobile Robots with An Uncalibrated Onboard Camera.pdf:pdf},
issn = {10834435},
journal = {IEEE/ASME Transactions on Mechatronics},
keywords = {Nonholonomic,projection homography matrix,uncalibrated camera,visual servo regulation,wheeled mobile robot},
number = {5},
pages = {2330--2342},
title = {{Visual Servo Regulation of Wheeled Mobile Robots with an Uncalibrated Onboard Camera}},
volume = {21},
year = {2016}
}
@article{GOTOU,
author = {Gotou, Akira and Fujimoto, Hiroshi and National, Yokohama},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/GOTOU, Fujimoto - Unknown - Visual Servoing for Flying Object Based on Realtime Distance Identification.pdf:pdf},
journal = {Robotics},
keywords = {flying object,multirate control,real-time distance identification,visual servoing},
number = {4},
pages = {0--3},
title = {{Visual Servoing for Flying Object Based on Realtime Distance Identification}}
}
@article{Silveira2009,
abstract = {To date, there exist only few works on the use of color images for visual servoing. Perhaps, this is due to the difficulties usually found to cope with illumination changes in these images. This paper presents new parametric models and optimization methods for robustly and directly registering color images. Direct methods refer to those that exploit the pixel intensities, without resorting to image features. We then show how a robust and generic visual servoing scheme can be constructed using the obtained optimal parameters. The proposed models ensure robustness to arbitrary illumination changes in color images, do not require prior knowledge (including the spectral ones) of the object, illuminants or camera, and naturally encompass gray-level images. Furthermore, the exploitation of all information within the images, even from areas where no features exist, allow the algorithm to achieve high levels of accuracy. Various results are reported to show that visual servoing can indeed be highly accurate and robust despite unknown objects and unknown imaging conditions.},
author = {Silveira, Geraldo and Malis, Ezio},
doi = {10.1109/IROS.2009.5354423},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/Visual servoing from robust direct color image registration.pdf:pdf},
isbn = {9781424438044},
journal = {2009 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2009},
pages = {5450--5455},
title = {{Visual servoing from robust direct color image registration}},
year = {2009}
}
@article{Bourquardez2007,
abstract = {In this paper, a visual servoing scheme is proposed to control an airplane during its landing. A linearized model of the airplane dynamics and decoupled visual features are used to build the control scheme. A desired trajectory which takes into account the airplane dynamic is designed. Coupling this trajectory and the control law enables the airplane to join its desired path. Then the airplane is controlled to follow the glide path, realize the flare manoeuvre and finally touchdown. Simulation results are obtained with a quite realistic flight simulator which is based on a non linear airplane dynamic model. They show that the airplane is able to land automatically by using visual data.},
author = {Bourquardez, Odile and Chaumette, Fran{\c{c}}ois},
doi = {10.1109/IROS.2007.4399216},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bourquardez, Inputs - 2007 - Visual Servoing of an Airplane.pdf:pdf},
isbn = {1424409128},
issn = {1424409128},
journal = {IEEE International Conference on Intelligent Robots and Systems},
number = {April},
pages = {1314--1319},
title = {{Visual servoing of an airplane for auto-landing}},
year = {2007}
}
@article{Inaba2003,
author = {Inaba, Noriyasu and Oda, Mitsushige and Hayashi, Masato},
doi = {10.2322/tjsass.46.173},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/ETSⅦ.pdf:pdf},
isbn = {0549-3811},
issn = {0549-3811},
journal = {Transactions of the Japan Society for Aeronautical and Space Sciences},
keywords = {on-orbit servicing,satellite capture,space robot},
number = {153},
pages = {173--179},
title = {{Visual Servoing of Space Robot for Autonomous Satellite Capture}},
volume = {46},
year = {2003}
}
@article{Nam,
author = {Nam, Le Tien},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nam - Unknown - Visual Servoing Using Out-of-View Observer Based on Image Moment.pdf:pdf},
number = {3},
pages = {1--6},
title = {{Visual Servoing Using Out-of-View Observer Based on Image Moment}}
}
@article{Hosoda1995,
abstract = {A camera, which is used as an artificial vision in visual servoing control, often has a zoom mechanism. A zoom mechanism cannot realize fast motion while an arm mechanism can, and it has only one degree of freedom. On the other hand the arm mechanism cannot cover wide range of change of images while the zoom mechanism can. In this paper, we propose a complementary visual servoing controller of zoom and arm mechanisms. First we discuss on the condition that camera position and zoom setting are considered as redundant. Then a visual servoing controller is proposed making use of complementary characteristics of the both mechanisms. To show the effectiveness of the proposed controller, experimental results are shown},
author = {Hosoda, K. and Moriyama, H. and Asada, M.},
doi = {10.1109/ROBOT.1995.525282},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hosoda, Moriyama, Asada - 1995 - Visual servoing utilizing zoom mechanism.pdf:pdf},
isbn = {0-7803-1965-6},
issn = {10504729},
journal = {Proceedings of 1995 IEEE International Conference on Robotics and Automation},
pages = {178--183},
publisher = {Ieee},
title = {{Visual servoing utilizing zoom mechanism}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=525282},
volume = {1},
year = {1995}
}
@article{,
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 1997 - 変 換 転 行 動 出.pdf:pdf},
title = {パ フ お よびフ ー リエ 変換 を用い た 回転 と平行移動の検出},
year = {1997}
}
@article{Hashimoto2009a,
author = {Hashimoto, Kouich},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Institute of Systems, Control and Information Engineers NII-Electronic Library Service(3).pdf:pdf},
journal = {Institute of Systems, Control and Information Engineers NII-Electronic Library Service},
number = {9},
pages = {411--416},
title = {ビジュアルサーボ-Ⅰ-ビジュアルサーボとは},
volume = {53},
year = {2009}
}
@article{,
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/ビジュアルサーボシステムの可視度.pdf:pdf},
pages = {8--9},
title = {ビジュアルサーボシステムの可視度}
}
@article{Hashimoto2009,
author = {Hashimoto, Koichi},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/橋本 浩一 - 2009 - ビジュアルフィードバック制御と今後.pdf:pdf},
journal = {日本ロボット学会誌},
number = {4},
pages = {400--404},
title = {ビジュアルフィードバック制御と今後},
volume = {27},
year = {2009}
}
@article{Song2010,
author = {Song, Wei and Minami, Mamoru},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Song, Minami - 2010 - 3-d ? 1 ? 2.pdf:pdf},
isbn = {0668506482},
journal = {日本ロボット学会誌},
keywords = {ga,motion-feedforward compensation,pose measurement,unit quaternion},
number = {5},
pages = {110--118},
title = {{フィードフォワード遺伝的認識法を用いた 3-Dビジュアルサーボ}},
volume = {28},
year = {1919}
}
@misc{2005,
abstract = {むだ時間系の制御 は難 しいと言われる.身 近な例では, シャワーの温度調節において温度調節の蛇口と出口の間に 時間差があ り,設 定に手間取ることがある.ま た,国 際電 話 では国内電話 と違ってわずかに間があ り,相 手が しゃべ らないと思って話すと,両 方で話 して しまうということを 経験する.な ぜむだ時間系の制御が難 しいかは参考文献3) を参照 してほ しい.フ ィー ドバ ック制御において応答特性, 外乱特性 を良 くするためにむだ時間系ではフィー ドバ ック ゲインを単純 に大 きくできないことにその難 しさがある. 本解説では,む だ時間要素のパディ近似,簡 単 なステップ 応答によるむだ時間系の同定法を紹介し,PID制 御構造 で むだ時間系にたい して効果的な,部 分的モデルマ ッチ ング 法,そ してむだ時間を積極的に考慮 したス ミス法,内 部モ デル制御を説明する.},
author = {阿部直人 and 延山英沢},
booktitle = {計測と制御},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/むだ時間システム入門1-伝達関数からのアプローチ-.pdf:pdf},
isbn = {1111111111},
keywords = {PID制 御(PID control),む だ 時 間 系(time delay system),ス ミ ス 法(Smith predictor),内 部 モ デ ル 制 御(internal model control).},
number = {11},
pages = {799--804},
title = {むだ時間システムの制御―入門から最新動向まで 《第1回》むだ時間システム入門1―伝達関数からのアプローチ―},
volume = {44},
year = {2005}
}
@article{Unknowna,
author = {浅田, 稔},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - {\$} O {\$} 8 {\$} a {\$} K NO ! {\$}{\$} 9 {\$} J {\$} o {\$} A3X = , G = NO {\$} r {\$} b {\$} D {\$} 3 {\$} H {\$} , I , MWIT2D7g {\$} H9M {\$} ( {\$} i {\%}{\%} W {\%} F {\%}{\#}{\%}.pdf:pdf},
pages = {6--9},
title = {ロボットの行動獲得のための視覚}
}
@article{Nosaka,
abstract = {生産工程の自動外観検査において,従来の2次元画像処理では検出が困難な欠け,凹み,膨らみ,反 り等の欠陥検出や奥行方向の寸法計測を可能とするため,位相シフト法により直接3次元形状を計測 する3次元外観検査システムを開発した。 このシステムのプロジェクタはワークの大きさに合わせて投影視野を変更できるレンズ交換式であり, かつLEDを光源に用いて長寿命化を実現している。また,信頼性評価関数を新たに考案して計測値の 信頼性を確保するとともに濃淡境界部の計測誤差を改善するため,プロジェクタを二つ用いる構成を導 入して計測誤差を96 ％軽減している。},
author = {Nosaka, Ken-ichiro and Araki, Hidekazu and Nakahara, Tomoharu},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/位相シフト法インライン３次元計測システム.pdf:pdf},
journal = {パナソニック電工技報},
number = {3},
pages = {29--34},
title = {位相シフト法インライン3次元外観検査システム},
volume = {57}
}
@misc{,
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - 位相限定相関法に基づく映像の高精度動き推定.pdf.pdf:pdf},
title = {位相限定相関法に基づく映像の高精度動き推定.pdf}
}
@article{POC2005a,
author = {長嶋, 聖 and 青木, 孝文 and 常田, るり子},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - 位相限定相関法に基づく電子顕微鏡画像の倍率推定アルゴリズム.pdf.pdf:pdf},
journal = {IEICE 信学技法},
pages = {19--24},
title = {位相限定相関法に基づく電子顕微鏡画像の倍率推定アルゴリズム.pdf},
year = {2005}
}
@article{,
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - 画像に基づくファジィ制御器を用いた 移動ロボットの制御.pdf:pdf},
title = {画像に基づくファジィ制御器を用いた 移動ロボットの制御}
}
@article{Unknown,
abstract = {SIFT特征匹配算法是目前国内外特征点匹配研究?域的?点与?点,其匹配能力??,可以?理?幅?像之??生平移、旋?、?射??情况下的匹配??,甚至在某?程度上?任意角度拍?的?像也具????定的特征匹配能力。?算法目前外文?料?多,但中文方面的介??少。?此我撰写了?篇文档,以?助国内的研究学者尽快入?,以最快的速度去体? SIFT算法的魅力},
author = {藤吉},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - No Title.pdf:pdf},
pages = {1--128},
title = {{画像局所特徴量と特定物体認識 - Siftと最近のアプローチ -}}
}
@article{,
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/KLTtracker{\_}kagami.pdf:pdf},
number = {1},
title = {画像追跡 (1) ― 特徴点の検出と追跡 ―},
year = {2008}
}
@article{Ueno,
author = {Ueno, Satoshi and Hashimoto, Masayuki and Yoneyama, Akio},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/建物画像検索のための特徴点選択手法の一検討.pdf:pdf},
pages = {2--3},
title = {{建物画像検索のための特徴点選択手法の一検討 A method for corresponding point selection for similar building image retrieval}}
}
@article{Histgram,
abstract = {本論文では,画像間を密かつ高精度に マッチングするための手法を検討する．画像マッチングは,画像処理・コンピュータビジョン・パターン認識などの分野において,重要な基本処理となっている．近年は,特に,特徴ベースマッチングの研究が盛んに行われており,SIFT (Scale-Invariant Feature Transform) が提案されて以来,さまざまなマッチング手法が研究されている． 本論文では,特徴ベースマッチングの特長である画像変形にロバストであることと,領域ベースマッチングの特長である密にマッチングできることを活かしたマッチング手法を提案する．具体的には,特徴ベースマッチング を用いて画像間の大きな変形を補正し,領域ベースマッチングを用いて密に対応付ける．一般に公開されている 標準画像を用いて,これまでに提案されている特徴ベースマッチングと性能を比較し,提案手法の有効性を実証する．},
author = {伊藤, 康一 and 高橋, 徹 and 孝文, 青木},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Histgram - Unknown - A Study of a High-Accuracy Image Matching Method Koichi ITO Toru TAKAHASHI Takafumi AOKI Graduate School of Informa.pdf:pdf},
journal = {第25回信号処理シンポジウム},
pages = {547--552},
title = {{高精度な画像マッチング手法の検討 A Study of a High-Accuracy Image Matching Method}},
year = {2010}
}
@article{Matching,
author = {Matching, Using High-accuracy Image},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Matching - Unknown - 高精度画像マッチングを用いた sar 衛星画像からの地表変位推定.pdf:pdf},
pages = {1--2},
title = {{高精度画像マッチングを用いた Sar 衛星画像からの地表変位推定}}
}
@article{,
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - ? ? ?? ?? ??.pdf:pdf},
title = {作業空間の非干渉化に基づいた ビジュアルサーボ系の新しい設計法}
}
@inproceedings{LeTien2010,
author = {{Le, Tien}, Nam and Fujimoto, Hiroshi},
booktitle = {IIC},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - No Title(2).pdf:pdf},
number = {144},
pages = {1--6},
title = {視野外オブザーバを用いたマルチレートビジュアルサーボ法の提案},
volume = {10},
year = {2010}
}
@article{Aoki,
author = {青木, 元伸 and 藤本, 博志 and 堀, 洋一 and 高橋, 太郎},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Aoki, Fujimoto - Unknown - Robust Control of Two-inertia System Based on Self Resonance Cancellation Disturbance Observer and Applicatio.pdf:pdf},
journal = {電気学会研究会資料. Iic, 産業計測制御研究会},
keywords = {パラメータ変動,ヒューマノイド,ロバスト制御,外乱オブザーバ,自己共振相殺制御,自己共振相殺外乱オブザーバ},
number = {171},
pages = {49--54},
title = {自己共振相殺外乱オブザーバを用いた2慣性系のロバスト制御とそのヒューマノイドロボットへの適用},
url = {http://ci.nii.ac.jp/naid/10031118496/},
volume = {2012},
year = {2012}
}
@article{Shiraishia,
author = {{白石 貴行} and {藤本 博志}},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shiraishi - Unknown - Vibration Suppression Position Control of Three-inertia Systems Using Self Resonance Cancellation Control.pdf:pdf},
journal = {電気学会研究会資料. Iic, 産業計測制御研究会},
number = {156},
pages = {89--94},
title = {自己共振相殺制御を用いた3慣性系の制振制御},
url = {http://ci.nii.ac.jp/naid/10029662522},
volume = {2011},
year = {2011}
}
@article{Pattern2013,
author = {Pattern, Robust and Method, Matching and Principal, Using and Analysis, Component and Edges, Image},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pattern et al. - 2013 - 主成分分析と画像エッジを用いたロバストかつ高速な パターンマッチング手法の開発.pdf:pdf},
pages = {1--4},
title = {主成分分析と画像エッジを用いたロバストかつ高速な パターンマッチング手法の開発},
year = {2013}
}
@misc{Unknownb,
author = {駒田, 諭},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - 推定画像特徴量を用いたビジュアルサーボ.pdf.pdf:pdf},
title = {推定画像特徴量を用いたロボットのビジュアルサーボ}
}
@article{Vehicle,
author = {森本, 雅和 and 金子, 弘樹 and 宮本, 直樹},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/visualservo/単眼車載カメラ.pdf:pdf},
title = {単眼車載カメラ映像からの装甲車両速度推定}
}
@article{N,
author = {N, F A H Fe H},
file = {:C$\backslash$:/Users/yoshi/Documents/堀藤本研究室/論文/2台のカメラを用いたマニピュレーの障害物回避軌道の生成.pdf:pdf},
pages = {1--8},
title = {二台のカメラを用いたマニピュレータの平面障害物回避軌道の作成}
}
@article{Words1996,
author = {Words, Key},
file = {:C$\backslash$:/Users/yoshi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Words - 1996 - NO {\%} 5 !!{\%}“{\%} O {\%} {\$} {\%} V {\%} j {\%} C {\%} I @) 8f7O YED.pdf:pdf},
keywords = {force servoing,hybrid control,on-line estimation,unknown environment,visual servoing},
number = {Xx},
pages = {1001--1006},
title = {未知環境内で動作するロボットの ビジュアルサーボ/力サーボハイブリッド制御系},
volume = {XX},
year = {1996}
}

@article{Chesi2004,
abstract = { A visual servoing strategy for keeping features in the field of view is proposed which consists of a switching among position-based control strategies and backward motion. In the absence of uncertainty on the extrinsic parameters, all features are kept in the field of view. Moreover, if the intrinsic parameters are also known, the trajectory length is minimized in the rotational space and, for some cases, also minimized in the translational space. Simulation results also show a certain degree of robustness against uncertainty on the intrinsic parameters.},
author = {Chesi, Graziano and Hashimoto, Koichi and Prattichizzo, Domenico and Vicino, Antonio},
doi = {10.1109/TRO.2004.829456},
file = {:C$\backslash$:/Users/yoshi/Dropbox/papers/Keeping Features in the Field of View in Eye-In-Hand Visual Servoing A Switching Approach.pdf:pdf},
isbn = {0-7803-7736-2},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Field of view,Point correspondences,Switching control,Visual servoing},
number = {5},
pages = {908--913},
title = {{Keeping features in the field of view in eye-in-hand visual servoing: A switching approach}},
volume = {20},
year = {2004}
}



@article{Lowe1999,
abstract = {An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds},
archivePrefix = {arXiv},
arxivId = {cs/0112017},
author = {Lowe, David G.},
doi = {10.1109/ICCV.1999.790410},
eprint = {0112017},
file = {:C$\backslash$:/Users/yoshi/Dropbox/papers/Object Recognition from Local Scale-Invariant Features.pdf:pdf},
isbn = {0-7695-0164-8},
issn = {0-7695-0164-8},
journal = {Proceedings of the Seventh IEEE International Conference on Computer Vision},
number = {[8},
pages = {1150--1157},
pmid = {15806121},
primaryClass = {cs},
title = {{Object recognition from local scale-invariant features}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=790410},
volume = {2},
year = {1999}
}
@article{Lowe2004,
abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
archivePrefix = {arXiv},
arxivId = {cs/0112017},
author = {Lowe, David G},
doi = {http://dx.doi.org/10.1023/B:VISI.0000029664.99615.94},
eprint = {0112017},
file = {:C$\backslash$:/Users/yoshi/Dropbox/papers/Distinctive Image Features from Scale-Invariant Keypoints.pdf:pdf},
isbn = {1568811012},
issn = {0920-5691},
journal = {Int'l Journal of Computer Vision},
pages = {91--11020042},
pmid = {20064111},
primaryClass = {cs},
title = {{Distinctive image features from scale invariant keypoints}},
url = {http://portal.acm.org/citation.cfm?id=996342},
volume = {60},
year = {2004}
}

@article{Bay2006,
abstract = {Abstract. In this paper, we present a novel scale- and rotation-invariant interest point detector and descriptor, coined SURF (Speeded Up Ro- bust Features). It approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (in casu, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper presents experimental results on a standard evaluation set, as well as on imagery obtained in the context of a real-life object recognition application. Both show SURF's strong performance.},
author = {Bay, Herbert and Tuytelaars, Tinne and {Van Gool}, Luc},
doi = {10.1007/11744023_32},
file = {:C$\backslash$:/Users/yoshi/Dropbox/papers/SURF Speeded Up Robust Features.pdf:pdf},
isbn = {3540338322},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {404--417},
pmid = {16081019},
title = {{SURF: Speeded up robust features}},
volume = {3951 LNCS},
year = {2006}
}

@article{Rublee2011,
abstract = {Feature matching is at the base of many computer vi-sion problems, such as object recognition or structure from motion. Current methods rely on costly descriptors for de-tection and matching. In this paper, we propose a very fast binary descriptor based on BRIEF, called ORB, which is rotation invariant and resistant to noise. We demonstrate through experiments how ORB is at two orders of magni-tude faster than SIFT, while performing as well in many situations. The efficiency is tested on several real-world ap-plications, including object detection and patch-tracking on a smart phone.},
author = {Rublee, Ethan and Rabaud, Vincent and Konolige, Kurt and Bradski, Gary},
doi = {10.1109/ICCV.2011.6126544},
file = {:C$\backslash$:/Users/yoshi/Dropbox/papers/ORB an efficient alternative to SIFT or SURF.pdf:pdf},
isbn = {9781457711015},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2564--2571},
pmid = {20033598},
title = {{ORB: An efficient alternative to SIFT or SURF}},
year = {2011}
}

@article{Alcantarilla2013,
abstract = {We propose a novel and fast multiscale feature detection and description approach that exploits the benefits of nonlinear scale spaces. Previous attempts to detect and describe features in nonlinear scale spaces are highly time consuming due to the computational burden of creating the nonlinear scale space. In this paper we propose to use recent numerical schemes called Fast Explicit Diffusion (FED) embedded in a pyramidal framework to dramatically speed-up feature detection in nonlinear scale spaces. In addition, we introduce a Modified-Local Difference Binary (M-LDB) descriptor that is highly efficient, exploits gradient information from the nonlinear scale space, is scale and rotation invariant and has low storage requirements. We present an extensive evaluation that shows the excellent compromise between speed and performance of our approach compared to state-of-the-art methods such as BRISK, ORB, SURF, SIFT and KAZE.},
author = {Alcantarilla, Pablo Fern{\'{a}}ndez and Nuevo, Jes{\'{u}}s and Bartoli, Adrien},
doi = {10.5244/C.27.13},
file = {:C$\backslash$:/Users/yoshi/Dropbox/papers/Fast Explicit Diffusion for Accelerated Features in Nonlinear Scale Spaces.pdf:pdf},
isbn = {1-901725-49-9},
journal = {British Machine Vision Conference},
pages = {13.1--13.11},
title = {{Fast Explicit Diffusion for Accelerated Features in Nonlinear Scale Spaces}},
url = {http://www.robesafe.com/personal/pablo.alcantarilla/kaze.html},
year = {2013}
}

@article{Hashimoto2014,
	author = {Hashimoto, Koichi},
	file = {:C$\backslash$:/Users/yoshi/Dropbox/papers/ロックオントラッキング顕微鏡.pdf:pdf},
	journal = {日本ロボット学会誌},
	number = {9},
	pages = {784--788},
	title = {ロックオントラッキング顕微鏡},
	volume = {32},
	year = {2014}
}

@article{hosoda1996,
	author = {細田 耕, 浅田 稔},
	journal = {日本ロボット学会誌},
	number = {2},
	pages = {159--165},
	title = { 構造やパラメータに関する先験的な知識	を必要としないフィードフォワード補償器を持つ適応型ビジュアルサーボ系の構成},
	volume = {14},
	year = {1996}
}

@article{endou1997,
	author = {遠藤 公誉, 田中 弘一, 荒川 賢一, 武川 直樹},
	journal = {日本ロボット学会誌},
	number = {4},
	pages = {565--572},
	title = { 構造やパラメータに関する先験的な知識	を必要としないフィードフォワード補償器を持つ適応型ビジュアルサーボ系の構成},
	volume = {15},
	year = {1997}
}


@article{Hashimoto1998,
	author = {橋本  浩一, 青木 篤人, 則次 俊郎},
	journal = {日本ロボット学会誌},
	number = {3},
	pages = {384--390},
	title = {冗長な特徴量に基づく視覚サーボ},
	volume = {16},
	year = {1998}
}

@inproceedings{Goto2008,
	author = {後藤彰, 藤本博志},
	booktitle = {平成20年電気学会産業計測制御研究会 IIC-08-30},
	title = {リアルタイム距離同定を用いた運動物体に対する6自由度ビジュアルサーボ法の提案},
	year = {2008}
}


@article{Mahony2005,
	abstract = {An image-based "eye-in-hand" visual servo-control design is proposed for underactuated rigid-body dynamics. The dynamic model considered is motivated by recent work on vertical takeoff and landing aerial robotic vehicles. The task considered is that of tracking parallel linear visual features. The proposed design exploits the geometry of the task considered and passivity-like properties of rigid-body dynamics to derive a control Lyapunov function using backstepping techniques.},
	author = {Mahony, Robert and Hamel, Tarek},
	doi = {10.1109/TRO.2004.835446},
	file = {:C$\backslash$:/Users/yoshi/Dropbox/papers/Image-Based Visual Servo Control of Aerial Robotic Systems Using Linear Image Features.pdf:pdf},
	isbn = {1552-3098},
	issn = {15523098},
	journal = {IEEE Transactions on Robotics},
	keywords = {Aerial robotic vehicle,Image-based visual servo (IBVS),Rigid-body dynamics,Underactuated systems},
	number = {2},
	pages = {227--239},
	title = {{Image-based visual servo control of aerial robotic systems using linear image features}},
	volume = {21},
	year = {2005}
}
